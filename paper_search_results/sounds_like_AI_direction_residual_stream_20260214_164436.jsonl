{"title": "Language Models Are Capable of Metacognitive Monitoring and Control of Their Internal Activations", "year": 2025, "authors": "Ji-An Li, Huadong Xiong, Robert C. Wilson, Marcelo G. Mattar, M. Benna", "url": "https://api.semanticscholar.org/CorpusId:278769631", "relevance": 3, "abstract": "Large language models (LLMs) can sometimes report the strategies they actually use to solve tasks, but they can also fail to do so. This suggests some degree of metacognition \u2014 the capacity to monitor one\u2019s own cognitive processes for subsequent reporting and self-control. Metacognitive abilities enhance AI capabilities but raise safety concerns, as models might obscure their internal processes to evade neural-activation-based oversight mechanisms designed to detect harmful behaviors. Given society\u2019s increased reliance on these models, it is critical that we understand the limits of their metacognitive abilities, particularly their ability to monitor their internal activations. To address this, we introduce a neuroscience-inspired neurofeedback paradigm designed to quantify the ability of LLMs to explicitly report and control their activation patterns. By presenting models with sentence-label pairs where labels correspond to sentence-elicited internal activations along specific directions in the neural representation space, we demonstrate that LLMs can learn to report and control these activations. The performance varies with several factors: the number of example pairs provided, the semantic interpretability of the target neural direction, and the variance explained by that direction. These results reveal a \u201cmetacognitive space\u201d with dimensionality much lower than the model\u2019s neural space, suggesting LLMs can monitor only a subset of their neural mechanisms. Our findings provide empirical evidence quantifying metacognitive capabilities in LLMs, with significant implications for AI safety.", "citations": 16}
{"title": "Refusal in Language Models Is Mediated by a Single Direction", "year": 2024, "authors": "Andy Arditi, Oscar Obeso, Aaquib Syed, Daniel Paleka, Nina Rimsky, Wes Gurnee, Neel Nanda", "url": "https://api.semanticscholar.org/CorpusId:270560489", "relevance": 3, "abstract": "Conversational large language models are fine-tuned for both instruction-following and safety, resulting in models that obey benign requests but refuse harmful ones. While this refusal behavior is widespread across chat models, its underlying mechanisms remain poorly understood. In this work, we show that refusal is mediated by a one-dimensional subspace, across 13 popular open-source chat models up to 72B parameters in size. Specifically, for each model, we find a single direction such that erasing this direction from the model's residual stream activations prevents it from refusing harmful instructions, while adding this direction elicits refusal on even harmless instructions. Leveraging this insight, we propose a novel white-box jailbreak method that surgically disables refusal with minimal effect on other capabilities. Finally, we mechanistically analyze how adversarial suffixes suppress propagation of the refusal-mediating direction. Our findings underscore the brittleness of current safety fine-tuning methods. More broadly, our work showcases how an understanding of model internals can be leveraged to develop practical methods for controlling model behavior.", "citations": 465}
{"title": "Transformer Dynamics: A neuroscientific approach to interpretability of large language models", "year": 2025, "authors": "Jesseba Fernando, Grigori Guitchounts", "url": "https://api.semanticscholar.org/CorpusId:276422389", "relevance": 3, "abstract": "As artificial intelligence models have exploded in scale and capability, understanding of their internal mechanisms remains a critical challenge. Inspired by the success of dynamical systems approaches in neuroscience, here we propose a novel framework for studying computations in deep learning systems. We focus on the residual stream (RS) in transformer models, conceptualizing it as a dynamical system evolving across layers. We find that activations of individual RS units exhibit strong continuity across layers, despite the RS being a non-privileged basis. Activations in the RS accelerate and grow denser over layers, while individual units trace unstable periodic orbits. In reduced-dimensional spaces, the RS follows a curved trajectory with attractor-like dynamics in the lower layers. These insights bridge dynamical systems theory and mechanistic interpretability, establishing a foundation for a\"neuroscience of AI\"that combines theoretical rigor with large-scale data analysis to advance our understanding of modern neural networks.", "citations": 6}
{"title": "Toward Preference-aligned Large Language Models via Residual-based Model Steering", "year": 2025, "authors": "Lucio La Cava, Andrea Tagarelli", "url": "https://api.semanticscholar.org/CorpusId:281675737", "relevance": 3, "abstract": "Preference alignment is a critical step in making Large Language Models (LLMs) useful and aligned with (human) preferences. Existing approaches such as Reinforcement Learning from Human Feedback or Direct Preference Optimization typically require curated data and expensive optimization over billions of parameters, and eventually lead to persistent task-specific models. In this work, we introduce Preference alignment of Large Language Models via Residual Steering (PaLRS), a training-free method that exploits preference signals encoded in the residual streams of LLMs. From as few as one hundred preference pairs, PaLRS extracts lightweight, plug-and-play steering vectors that can be applied at inference time to push models toward preferred behaviors. We evaluate PaLRS on various small-to-medium-scale open-source LLMs, showing that PaLRS-aligned models achieve consistent gains on mathematical reasoning and code generation benchmarks while preserving baseline general-purpose performance. Moreover, when compared to DPO-aligned models, they perform better with huge time savings. Our findings highlight that PaLRS offers an effective, much more efficient and flexible alternative to standard preference optimization pipelines, offering a training-free, plug-and-play mechanism for alignment with minimal data.", "citations": 0}
{"title": "No Training Wheels: Steering Vectors for Bias Correction at Inference Time", "year": 2025, "authors": "Aviral Gupta, Armaan Sethi, Ameesh Sethi", "url": "https://www.semanticscholar.org/paper/b90b7a35416671541b2ae362999f3c581dcb89fa", "relevance": 3, "abstract": "Neural network classifiers trained on datasets with uneven group representation often inherit class biases and learn spurious correlations. These models may perform well on average but consistently fail on atypical groups. For example, in hair color classification, datasets may over-represent females with blond hair, reinforcing stereotypes. Although various algorithmic and data-centric methods have been proposed to address such biases, they often require retraining or significant compute. In this work, we propose a cheap, training-free method inspired by steering vectors used to edit behaviors in large language models. We compute the difference in mean activations between majority and minority groups to define a\"bias vector,\"which we subtract from the model's residual stream. This leads to reduced classification bias and improved worst-group accuracy. We explore multiple strategies for extracting and applying these vectors in transformer-like classifiers, showing that steering vectors, traditionally used in generative models, can also be effective in classification. More broadly, we showcase an extremely cheap, inference time, training free method to mitigate bias in classification models.", "citations": 1}
{"title": "Inference-Time Intervention: Eliciting Truthful Answers from a Language Model", "year": 2023, "authors": "Kenneth Li, Oam Patel, Fernanda Vi'egas, H. Pfister, M. Wattenberg", "url": "https://www.semanticscholar.org/paper/405f8f5f1c6df1b3343c812832479aad5180b65f", "relevance": 3, "abstract": "We introduce Inference-Time Intervention (ITI), a technique designed to enhance the\"truthfulness\"of large language models (LLMs). ITI operates by shifting model activations during inference, following a set of directions across a limited number of attention heads. This intervention significantly improves the performance of LLaMA models on the TruthfulQA benchmark. On an instruction-finetuned LLaMA called Alpaca, ITI improves its truthfulness from 32.5% to 65.1%. We identify a tradeoff between truthfulness and helpfulness and demonstrate how to balance it by tuning the intervention strength. ITI is minimally invasive and computationally inexpensive. Moreover, the technique is data efficient: while approaches like RLHF require extensive annotations, ITI locates truthful directions using only few hundred examples. Our findings suggest that LLMs may have an internal representation of the likelihood of something being true, even as they produce falsehoods on the surface.", "citations": 880}
{"title": "A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO and Toxicity", "year": 2024, "authors": "Andrew Lee, Xiaoyan Bai, I. Pres, Martin Wattenberg, Jonathan K. Kummerfeld, Rada Mihalcea", "url": "https://www.semanticscholar.org/paper/26b2adbe089ea36617c3ec0aa009319929da0550", "relevance": 3, "abstract": "While alignment algorithms are now commonly used to tune pre-trained language models towards a user's preferences, we lack explanations for the underlying mechanisms in which models become ``aligned'', thus making it difficult to explain phenomena like jailbreaks. In this work we study a popular algorithm, direct preference optimization (DPO), and the mechanisms by which it reduces toxicity. Namely, we first study how toxicity is represented and elicited in a pre-trained language model, GPT2-medium. We then apply DPO with a carefully crafted pairwise dataset to reduce toxicity. We examine how the resulting model averts toxic outputs, and find that capabilities learned from pre-training are not removed, but rather bypassed. We use this insight to demonstrate a simple method to un-align the model, reverting it back to its toxic behavior.", "citations": 165}
{"title": "Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small", "year": 2022, "authors": "Kevin Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, J. Steinhardt", "url": "https://www.semanticscholar.org/paper/6edd112383ad494f5f2eba72b6f4ffae122ce61f", "relevance": 3, "abstract": "Research in mechanistic interpretability seeks to explain behaviors of machine learning models in terms of their internal components. However, most previous work either focuses on simple behaviors in small models, or describes complicated behaviors in larger models with broad strokes. In this work, we bridge this gap by presenting an explanation for how GPT-2 small performs a natural language task called indirect object identification (IOI). Our explanation encompasses 26 attention heads grouped into 7 main classes, which we discovered using a combination of interpretability approaches relying on causal interventions. To our knowledge, this investigation is the largest end-to-end attempt at reverse-engineering a natural behavior\"in the wild\"in a language model. We evaluate the reliability of our explanation using three quantitative criteria--faithfulness, completeness and minimality. Though these criteria support our explanation, they also point to remaining gaps in our understanding. Our work provides evidence that a mechanistic understanding of large ML models is feasible, opening opportunities to scale our understanding to both larger models and more complex tasks.", "citations": 823}
{"title": "Computational Basis of LLM's Decision Making in Social Simulation", "year": 2025, "authors": "Ji Ma", "url": "https://api.semanticscholar.org/CorpusId:277824488", "relevance": 3, "abstract": "Large language models (LLMs) increasingly serve as human-like decision-making agents in social science and applied settings. These LLM-agents are typically assigned human-like characters and placed in real-life contexts. However, how these characters and contexts shape an LLM's behavior remains underexplored. This study proposes and tests methods for probing, quantifying, and modifying an LLM's internal representations in a Dictator Game, a classic behavioral experiment on fairness and prosocial behavior. We extract ``vectors of variable variations''(e.g., ``male''to ``female'') from the LLM's internal state. Manipulating these vectors during the model's inference can substantially alter how those variables relate to the model's decision-making. This approach offers a principled way to study and regulate how social concepts can be encoded and engineered within transformer-based models, with implications for alignment, debiasing, and designing AI agents for social simulations in both academic and commercial applications, strengthening sociological theory and measurement.", "citations": 0}
{"title": "Analyzing Transformers in Embedding Space", "year": 2022, "authors": "Guy Dar, Mor Geva, Ankit Gupta, Jonathan Berant", "url": "https://www.semanticscholar.org/paper/c7fa5c2172a4624d6baa91e66344e4520d3028ad", "relevance": 3, "abstract": "Understanding Transformer-based models has attracted significant attention, as they lie at the heart of recent technological advances across machine learning. While most interpretability methods rely on running models over inputs, recent work has shown that a zero-pass approach, where parameters are interpreted directly without a forward/backward pass is feasible for some Transformer parameters, and for two-layer attention networks. In this work, we present a theoretical analysis where all parameters of a trained Transformer are interpreted by projecting them into the embedding space, that is, the space of vocabulary items they operate on. We derive a simple theoretical framework to support our arguments and provide ample evidence for its validity. First, an empirical analysis showing that parameters of both pretrained and fine-tuned models can be interpreted in embedding space. Second, we present two applications of our framework: (a) aligning the parameters of different models that share a vocabulary, and (b) constructing a classifier without training by \u201ctranslating\u201d the parameters of a fine-tuned classifier to parameters of a different model that was only pretrained. Overall, our findings open the door to interpretation methods that, at least in part, abstract away from model specifics and operate in the embedding space only.", "citations": 127}
{"title": "Effectively Steer LLM To Follow Preference via Building Confident Directions", "year": 2025, "authors": "Bingqing Song, Boran Han, Shuai Zhang, Hao Wang, Haoyang Fang, Bonan Min, Yuyang Wang, Mingyi Hong", "url": "https://api.semanticscholar.org/CorpusId:276782038", "relevance": 3, "abstract": "Having an LLM that aligns with human preferences is essential for accommodating individual needs, such as maintaining writing style or generating specific topics of interest. The majority of current alignment methods rely on fine-tuning or prompting, which can be either costly or difficult to control. Model steering algorithms, which modify the model output by constructing specific steering directions, are typically easy to implement and optimization-free. However, their capabilities are typically limited to steering the model into one of the two directions (i.e., bidirectional steering), and there has been no theoretical understanding to guarantee their performance. In this work, we propose a theoretical framework to understand and quantify the model steering methods. Inspired by the framework, we propose a confident direction steering method (CONFST) that steers LLMs via modifying their activations at inference time. More specifically, CONFST builds a confident direction that is closely aligned with users' preferences, and this direction is then added to the activations of the LLMs to effectively steer the model output. Our approach offers three key advantages over popular bidirectional model steering methods: 1) It is more powerful, since multiple (i.e. more than two) users' preferences can be aligned simultaneously; 2) It is simple to implement, since there is no need to determine which layer to add the steering vector to; 3) No explicit user instruction is required. We validate our method on GPT-2 XL (1.5B), Mistral (7B) and Gemma-it (9B) models for tasks that require shifting the output of LLMs across various topics and styles, achieving superior performance over competing methods.", "citations": 8}
{"title": "The Reasoning-Memorization Interplay in Language Models Is Mediated by a Single Direction", "year": 2025, "authors": "Yihuai Hong, Dian Zhou, Meng Cao, Lei Yu, Zhijing Jin", "url": "https://api.semanticscholar.org/CorpusId:277451917", "relevance": 3, "abstract": "Large language models (LLMs) excel on a variety of reasoning benchmarks, but previous studies suggest they sometimes struggle to generalize to unseen questions, potentially due to over-reliance on memorized training examples. However, the precise conditions under which LLMs switch between reasoning and memorization during text generation remain unclear. In this work, we provide a mechanistic understanding of LLMs' reasoning-memorization dynamics by identifying a set of linear features in the model's residual stream that govern the balance between genuine reasoning and memory recall. These features not only distinguish reasoning tasks from memory-intensive ones but can also be manipulated to causally influence model performance on reasoning tasks. Additionally, we show that intervening in these reasoning features helps the model more accurately activate the most relevant problem-solving capabilities during answer generation. Our findings offer new insights into the underlying mechanisms of reasoning and memory in LLMs and pave the way for the development of more robust and interpretable generative AI systems.", "citations": 15}
{"title": "Sparse Autoencoders Find Highly Interpretable Features in Language Models", "year": 2023, "authors": "Hoagy Cunningham, Aidan Ewart, Logan Riggs Smith, R. Huben, Lee Sharkey", "url": "https://www.semanticscholar.org/paper/edb548fe7574d99454b352ffdb61bca93c3072ba", "relevance": 3, "abstract": "One of the roadblocks to a better understanding of neural networks' internals is \\textit{polysemanticity}, where neurons appear to activate in multiple, semantically distinct contexts. Polysemanticity prevents us from identifying concise, human-understandable explanations for what neural networks are doing internally. One hypothesised cause of polysemanticity is \\textit{superposition}, where neural networks represent more features than they have neurons by assigning features to an overcomplete set of directions in activation space, rather than to individual neurons. Here, we attempt to identify those directions, using sparse autoencoders to reconstruct the internal activations of a language model. These autoencoders learn sets of sparsely activating features that are more interpretable and monosemantic than directions identified by alternative approaches, where interpretability is measured by automated methods. Moreover, we show that with our learned set of features, we can pinpoint the features that are causally responsible for counterfactual behaviour on the indirect object identification task \\citep{wang2022interpretability} to a finer degree than previous decompositions. This work indicates that it is possible to resolve superposition in language models using a scalable, unsupervised method. Our method may serve as a foundation for future mechanistic interpretability work, which we hope will enable greater model transparency and steerability.", "citations": 865}
{"title": "From Directions to Cones: Exploring Multidimensional Representations of Propositional Facts in LLMs", "year": 2025, "authors": "Stanley Yu, Vaidehi Bulusu, Oscar Yasunaga, C. Lau, Cole Blondin, Sean O'Brien, Kevin Zhu, Vasu Sharma", "url": "https://api.semanticscholar.org/CorpusId:278959856", "relevance": 3, "abstract": "Large Language Models (LLMs) exhibit strong conversational abilities but often generate falsehoods. Prior work suggests that the truthfulness of simple propositions can be represented as a single linear direction in a model's internal activations, but this may not fully capture its underlying geometry. In this work, we extend the concept cone framework, recently introduced for modeling refusal, to the domain of truth. We identify multi-dimensional cones that causally mediate truth-related behavior across multiple LLM families. Our results are supported by three lines of evidence: (i) causal interventions reliably flip model responses to factual statements, (ii) learned cones generalize across model architectures, and (iii) cone-based interventions preserve unrelated model behavior. These findings reveal the richer, multidirectional structure governing simple true/false propositions in LLMs and highlight concept cones as a promising tool for probing abstract behaviors.", "citations": 2}
{"title": "Circuit Component Reuse Across Tasks in Transformer Language Models", "year": 2023, "authors": "Jack Merullo, Carsten Eickhoff, Ellie Pavlick", "url": "https://www.semanticscholar.org/paper/c0d9a48547d728dd320b453b01a0ab1ce2f96098", "relevance": 3, "abstract": "Recent work in mechanistic interpretability has shown that behaviors in language models can be successfully reverse-engineered through circuit analysis. A common criticism, however, is that each circuit is task-specific, and thus such analysis cannot contribute to understanding the models at a higher level. In this work, we present evidence that insights (both low-level findings about specific heads and higher-level findings about general algorithms) can indeed generalize across tasks. Specifically, we study the circuit discovered in Wang et al. (2022) for the Indirect Object Identification (IOI) task and 1.) show that it reproduces on a larger GPT2 model, and 2.) that it is mostly reused to solve a seemingly different task: Colored Objects (Ippolito&Callison-Burch, 2023). We provide evidence that the process underlying both tasks is functionally very similar, and contains about a 78% overlap in in-circuit attention heads. We further present a proof-of-concept intervention experiment, in which we adjust four attention heads in middle layers in order to 'repair' the Colored Objects circuit and make it behave like the IOI circuit. In doing so, we boost accuracy from 49.6% to 93.7% on the Colored Objects task and explain most sources of error. The intervention affects downstream attention heads in specific ways predicted by their interactions in the IOI circuit, indicating that this subcircuit behavior is invariant to the different task inputs. Overall, our results provide evidence that it may yet be possible to explain large language models' behavior in terms of a relatively small number of interpretable task-general algorithmic building blocks and computational components.", "citations": 99}
{"title": "Robust LLM safeguarding via refusal feature adversarial training", "year": 2024, "authors": "Lei Yu, Virginie Do, Karen Hambardzumyan, Nicola Cancedda", "url": "https://www.semanticscholar.org/paper/57b7415577c0b81796de282b6c952e6cc5a6241e", "relevance": 3, "abstract": "Large language models (LLMs) are vulnerable to adversarial attacks that can elicit harmful responses. Defending against such attacks remains challenging due to the opacity of jailbreaking mechanisms and the high computational cost of training LLMs robustly. We demonstrate that adversarial attacks share a universal mechanism for circumventing LLM safeguards that works by ablating a dimension in the residual stream embedding space called the refusal feature. We further show that the operation of refusal feature ablation (RFA) approximates the worst-case perturbation of offsetting model safety. Based on these findings, we propose Refusal Feature Adversarial Training (ReFAT), a novel algorithm that efficiently performs LLM adversarial training by simulating the effect of input-level attacks via RFA. Experiment results show that ReFAT significantly improves the robustness of three popular LLMs against a wide range of adversarial attacks, with considerably less computational overhead compared to existing adversarial training methods.", "citations": 45}
{"title": "Turning the Spell Around: Lightweight Alignment Amplification via Rank-One Safety Injection", "year": 2025, "authors": "Harethah Abu Shairah, Hasan Hammoud, G. Turkiyyah, Bernard Ghanem", "url": "https://api.semanticscholar.org/CorpusId:280950135", "relevance": 3, "abstract": "Safety alignment in Large Language Models (LLMs) often involves mediating internal representations to refuse harmful requests. Recent research has demonstrated that these safety mechanisms can be bypassed by ablating or removing specific representational directions within the model. In this paper, we propose the opposite approach: Rank-One Safety Injection (ROSI), a white-box method that amplifies a model's safety alignment by permanently steering its activations toward the refusal-mediating subspace. ROSI operates as a simple, fine-tuning-free rank-one weight modification applied to all residual stream write matrices. The required safety direction can be computed from a small set of harmful and harmless instruction pairs. We show that ROSI consistently increases safety refusal rates - as evaluated by Llama Guard 3 - while preserving the utility of the model on standard benchmarks such as MMLU, HellaSwag, and Arc. Furthermore, we show that ROSI can also re-align'uncensored'models by amplifying their own latent safety directions, demonstrating its utility as an effective last-mile safety procedure. Our results suggest that targeted, interpretable weight steering is a cheap and potent mechanism to improve LLM safety, complementing more resource-intensive fine-tuning paradigms.", "citations": 2}
{"title": "A Single Direction of Truth: An Observer Model's Linear Residual Probe Exposes and Steers Contextual Hallucinations", "year": 2025, "authors": "Charles O'Neill, Slava Chalnev, Chi Chi Zhao, Max Kirkby, Mudith Jayasekara", "url": "https://www.semanticscholar.org/paper/576531260abc89ed3a08a6ce552706e7860e9394", "relevance": 3, "abstract": "Contextual hallucinations -- statements unsupported by given context -- remain a significant challenge in AI. We demonstrate a practical interpretability insight: a generator-agnostic observer model detects hallucinations via a single forward pass and a linear probe on its residual stream. This probe isolates a single, transferable linear direction separating hallucinated from faithful text, outperforming baselines by 5-27 points and showing robust mid-layer performance across Gemma-2 models (2B to 27B). Gradient-times-activation localises this signal to sparse, late-layer MLP activity. Critically, manipulating this direction causally steers generator hallucination rates, proving its actionability. Our results offer novel evidence of internal, low-dimensional hallucination tracking linked to specific MLP sub-circuits, exploitable for detection and mitigation. We release the 2000-example ContraTales benchmark for realistic assessment of such solutions.", "citations": 4}
{"title": "Interpreting Key Mechanisms of Factual Recall in Transformer-Based Language Models", "year": 2024, "authors": "Ang Lv, Kaiyi Zhang, Yuhan Chen, Yulong Wang, Lifeng Liu, Ji-Rong Wen, Jian Xie, Rui Yan", "url": "https://api.semanticscholar.org/CorpusId:268732668", "relevance": 3, "abstract": "In this paper, we delve into several mechanisms employed by Transformer-based language models (LLMs) for factual recall tasks. We outline a pipeline consisting of three major steps: (1) Given a prompt ``The capital of France is,'' task-specific attention heads extract the topic token, such as ``France,'' from the context and pass it to subsequent MLPs. (2) As attention heads' outputs are aggregated with equal weight and added to the residual stream, the subsequent MLP acts as an ``activation,'' which either erases or amplifies the information originating from individual heads. As a result, the topic token ``France'' stands out in the residual stream. (3) A deep MLP takes ``France'' and generates a component that redirects the residual stream towards the direction of the correct answer, i.e., ``Paris.'' This procedure is akin to applying an implicit function such as ``get\\_capital($X$),'' and the argument $X$ is the topic token information passed by attention heads. To achieve the above quantitative and qualitative analysis for MLPs, we proposed a novel analytic method aimed at decomposing the outputs of the MLP into components understandable by humans. Additionally, we observed a universal anti-overconfidence mechanism in the final layer of models, which suppresses correct predictions. We mitigate this suppression by leveraging our interpretation to improve factual recall confidence. The above interpretations are evaluated across diverse tasks spanning various domains of factual knowledge, using various language models from the GPT-2 families, 1.3B OPT, up to 7B Llama-2, and in both zero- and few-shot setups.", "citations": 24}
{"title": "An Adversarial Example for Direct Logit Attribution: Memory Management in GELU-4L", "year": 2023, "authors": "James Dao, Yeu-Tong Lau, Can Rager, Jett Janiak", "url": "https://www.semanticscholar.org/paper/ca0a105d631a5cc5873e9e2dfa09c5fdfe81a8e2", "relevance": 3, "abstract": "Prior work suggests that language models manage the limited bandwidth of the residual stream through a \u201cmemory management\u201d mechanism, where certain attention heads and MLP layers clear residual stream directions set by earlier layers. Our study provides concrete evidence for this erasure phenomenon in a 4-layer transformer, identifying heads that consistently remove the output of earlier heads. We further demonstrate that direct logit attribution (DLA), a common technique for interpreting the output of intermediate transformer layers, can show misleading results by not accounting for erasure.", "citations": 5}
{"title": "Revisiting Residual Connections: Orthogonal Updates for Stable and Efficient Deep Networks", "year": 2025, "authors": "Giyeong Oh, Woohyun Cho, Siyeol Kim, Suhwan Choi, Younjae Yu", "url": "https://api.semanticscholar.org/CorpusId:278741008", "relevance": 3, "abstract": "Residual connections are pivotal for deep neural networks, enabling greater depth by mitigating vanishing gradients. However, in standard residual updates, the module's output is directly added to the input stream. This can lead to updates that predominantly reinforce or modulate the existing stream direction, potentially underutilizing the module's capacity for learning entirely novel features. In this work, we introduce Orthogonal Residual Update: we decompose the module's output relative to the input stream and add only the component orthogonal to this stream. This design aims to guide modules to contribute primarily new representational directions, fostering richer feature learning while promoting more efficient training. We demonstrate that our orthogonal update strategy improves generalization accuracy and training stability across diverse architectures (ResNetV2, Vision Transformers) and datasets (CIFARs, TinyImageNet, ImageNet-1k), achieving, for instance, a +3.78 pp top-1 accuracy gain for ViT-B on ImageNet-1k.", "citations": 0}
{"title": "Convergent Linear Representations of Emergent Misalignment", "year": 2025, "authors": "Anna Soligo, Edward Turner, Senthooran Rajamanoharan, Neel Nanda", "url": "https://www.semanticscholar.org/paper/8b97874ce9904c9f5e712054d24a8b45cc6fed5e", "relevance": 3, "abstract": "Fine-tuning large language models on narrow datasets can cause them to develop broadly misaligned behaviours: a phenomena known as emergent misalignment. However, the mechanisms underlying this misalignment, and why it generalizes beyond the training domain, are poorly understood, demonstrating critical gaps in our knowledge of model alignment. In this work, we train and study a minimal model organism which uses just 9 rank-1 adapters to emergently misalign Qwen2.5-14B-Instruct. Studying this, we find that different emergently misaligned models converge to similar representations of misalignment. We demonstrate this convergence by extracting a'misalignment direction'from one fine-tuned model's activations, and using it to effectively ablate misaligned behaviour from fine-tunes using higher dimensional LoRAs and different datasets. Leveraging the scalar hidden state of rank-1 LoRAs, we further present a set of experiments for directly interpreting the fine-tuning adapters, showing that six contribute to general misalignment, while two specialise for misalignment in just the fine-tuning domain. Emergent misalignment is a particularly salient example of undesirable and unexpected model behaviour and by advancing our understanding of the mechanisms behind it, we hope to move towards being able to better understand and mitigate misalignment more generally.", "citations": 17}
{"title": "AI-Generated Image Detection using a Cross-Attention Enhanced Dual-Stream Network", "year": 2023, "authors": "Ziyi Xi, Wenmin Huang, Kangkang Wei, Weiqi Luo, Peijia Zheng", "url": "https://api.semanticscholar.org/CorpusId:259137544", "relevance": 3, "abstract": "With the rapid evolution of AI Generated Content (AIGC), forged images produced through this technology are inherently more deceptive and require less human intervention compared to traditional Computer-generated Graphics (CG). However, owing to the disparities between CG and AIGC, conventional CG detection methods tend to be inadequate in identifying AIGC-produced images. To address this issue, our research concentrates on the text-to-image generation process in AIGC. Initially, we first assemble two text-to-image databases utilizing two distinct AI systems, DALL\u2022E2 and DreamStudio. Aiming to holistically capture the inherent anomalies produced by AIGC, we develope a robust dual-stream network comprised of a residual stream and a content stream. The former employs the Spatial Rich Model (SRM) to meticulously extract various texture information from images, while the latter seeks to capture additional forged traces in low frequency, thereby extracting complementary information that the residual stream may overlook. To enhance the information exchange between these two streams, we incorporate a cross multi-head attention mechanism. Numerous comparative experiments are performed on both databases, and the results show that our detection method consistently outperforms traditional CG detection techniques across a range of image resolutions. Moreover, our method exhibits superior performance through a series of robustness tests and cross-database experiments. When applied to widely recognized traditional CG benchmarks such as SPL2018 and DsTok, our approach significantly exceeds the capabilities of other existing methods in the field of CG detection.", "citations": 35}
{"title": "Fine-Grained control over Music Generation with Activation Steering", "year": 2025, "authors": "Dipanshu Panda, Jayden Koshy Joe, R. HarshithM, Swathi Narashiman, Pranay Mathur, Anish Veerakumar, Aniruddh Krishna, A. Keerthiharan", "url": "https://www.semanticscholar.org/paper/e3851b47a1c153bcadbd1bd96814131d9675a7a1", "relevance": 3, "abstract": "We present a method for fine-grained control over music generation through inference-time interventions on an autoregressive generative music transformer called MusicGen. Our approach enables timbre transfer, style transfer, and genre fusion by steering the residual stream using weights of linear probes trained on it, or by steering the attention layer activations in a similar manner. We observe that modelling this as a regression task provides improved performance, hypothesizing that the mean-squared-error better preserve meaningful directional information in the activation space. Combined with the global conditioning offered by text prompts in MusicGen, our method provides both global and local control over music generation. Audio samples illustrating our method are available at our demo page.", "citations": 0}
{"title": "Geometry of Decision Making in Language Models", "year": 2025, "authors": "Abhinav Joshi, Divyanshu Bhatt, Ashutosh Modi", "url": "https://www.semanticscholar.org/paper/18f2c9d50cf832ef409971d8351bd0a6a6c26c0e", "relevance": 3, "abstract": "Large Language Models (LLMs) show strong generalization across diverse tasks, yet the internal decision-making processes behind their predictions remain opaque. In this work, we study the geometry of hidden representations in LLMs through the lens of \\textit{intrinsic dimension} (ID), focusing specifically on decision-making dynamics in a multiple-choice question answering (MCQA) setting. We perform a large-scale study, with 28 open-weight transformer models and estimate ID across layers using multiple estimators, while also quantifying per-layer performance on MCQA tasks. Our findings reveal a consistent ID pattern across models: early layers operate on low-dimensional manifolds, middle layers expand this space, and later layers compress it again, converging to decision-relevant representations. Together, these results suggest LLMs implicitly learn to project linguistic inputs onto structured, low-dimensional manifolds aligned with task-specific decisions, providing new geometric insights into how generalization and reasoning emerge in language models.", "citations": 1}
{"title": "Safety Alignment Should Be Made More Than Just A Few Attention Heads", "year": 2025, "authors": "Chao Huang, Zefeng Zhang, Juewei Yue, Quangang Li, Chuang Zhang, Tingwen Liu", "url": "https://www.semanticscholar.org/paper/e023b181734934ae643eeb4c31f5d9191e6cd940", "relevance": 3, "abstract": "Current safety alignment for large language models(LLMs) continues to present vulnerabilities, given that adversarial prompting can effectively bypass their safety measures.Our investigation shows that these safety mechanisms predominantly depend on a limited subset of attention heads: removing or ablating these heads can severely compromise model safety. To identify and evaluate these safety-critical components, we introduce RDSHA, a targeted ablation method that leverages the model's refusal direction to pinpoint attention heads mostly responsible for safety behaviors. Further analysis shows that existing jailbreak attacks exploit this concentration by selectively bypassing or manipulating these critical attention heads. To address this issue, we propose AHD, a novel training strategy designed to promote the distributed encoding of safety-related behaviors across numerous attention heads. Experimental results demonstrate that AHD successfully distributes safety-related capabilities across more attention heads. Moreover, evaluations under several mainstream jailbreak attacks show that models trained with AHD exhibit considerably stronger safety robustness, while maintaining overall functional utility.", "citations": 0}
{"title": "Designing Role Vectors to Improve LLM Inference Behaviour", "year": 2025, "authors": "Daniele Potert\u00ec, Andrea Seveso, Fabio Mercorio", "url": "https://www.semanticscholar.org/paper/7e9395ce5521944cde25d3cea8986cd6394f0db9", "relevance": 3, "abstract": "The influence of personas on Large Language Models (LLMs) has been widely studied, yet their direct impact on performance remains uncertain. This work explores a novel approach to guiding LLM behaviour through role vectors, an alternative to persona-based prompting. We construct 29 role vectors derived from model activations and evaluate their impact on benchmark performance across multiple domains. Our analysis investigates whether these vectors can effectively steer models toward domain-specific expertise. We measure two key interventions: (i) activation addition, which reinforces role-specific directions, and (ii) directional ablation, which removes them. Results on well-established benchmarks indicate that role vectors do, in fact, influence model behaviour, improving task performance in relevant domains while marginally affecting unrelated tasks. This, in turn, suggests that manipulating internal model representations has a greater impact on outcomes than persona-based prompting.", "citations": 3}
{"title": "Anchor Attention, Small Cache: Code Generation With Large Language Models", "year": 2024, "authors": "Xiangyu Zhang, Yu Zhou, Guang Yang, Harald C. Gall, Taolue Chen", "url": "https://api.semanticscholar.org/CorpusId:273963303", "relevance": 3, "abstract": "The development of large language models (LLMs) has revolutionized automated code generation. However, their high demand of computation resources has hindered a broader deployment and raised environmental concerns. A common strategy for diminishing computational demands is to cache Key-Value (KV) states from the attention mechanism which is adopted predominately by mainstream LLMs. It can mitigate the need of repeated attention computations, but brings significant memory overhead. Current practices in NLP often use sparse attention which may, unfortunately, lead to substantial inaccuracies, or hallucinations, in code generation tasks. In this paper, we analyze the attention weights distribution within code generation models via an empirical study, uncovering a sparsity pattern, i.e., the aggregation of information at specific anchor points. Based on this observation, we propose a novel approach, AnchorCoder, which features token-wise anchor attention designed to extract and compress the contextual information, and layer-wise anchor attention enabling cross-layer communication to mitigate the issue of excessive superposition caused by the compression. The extensive experiments across multiple benchmark datasets confirm the effectiveness of AnchorCoder, which can consistently achieve a significant (at least 70%) reduction in KV cache requirements, while preserving the majority of model\u2019s performance.", "citations": 3}
{"title": "Residual Stream Analysis with Multi-Layer SAEs", "year": 2024, "authors": "Tim Lawson, Lucy Farnik, Conor Houghton, Laurence Aitchison", "url": "https://www.semanticscholar.org/paper/f2edde8b5c985c0c7ad57e49fc2eae8c4e58d163", "relevance": 3, "abstract": "Sparse autoencoders (SAEs) are a promising approach to interpreting the internal representations of transformer language models. However, SAEs are usually trained separately on each transformer layer, making it difficult to use them to study how information flows across layers. To solve this problem, we introduce the multi-layer SAE (MLSAE): a single SAE trained on the residual stream activation vectors from every transformer layer. Given that the residual stream is understood to preserve information across layers, we expected MLSAE latents to 'switch on' at a token position and remain active at later layers. Interestingly, we find that individual latents are often active at a single layer for a given token or prompt, but the layer at which an individual latent is active may differ for different tokens or prompts. We quantify these phenomena by defining a distribution over layers and considering its variance. We find that the variance of the distributions of latent activations over layers is about two orders of magnitude greater when aggregating over tokens compared with a single token. For larger underlying models, the degree to which latents are active at multiple layers increases, which is consistent with the fact that the residual stream activation vectors at adjacent layers become more similar. Finally, we relax the assumption that the residual stream basis is the same at every layer by applying pre-trained tuned-lens transformations, but our findings remain qualitatively similar. Our results represent a new approach to understanding how representations change as they flow through transformers. We release our code to train and analyze MLSAEs at https://github.com/tim-lawson/mlsae.", "citations": 11}
{"title": "Analysing the Residual Stream of Language Models Under Knowledge Conflicts", "year": 2024, "authors": "Yu Zhao, Xiaotang Du, Giwon Hong, Aryo Pradipta Gema, Alessio Devoto, Hongru Wang, Xuanli He, Kam-Fai Wong, Pasquale Minervini", "url": "https://www.semanticscholar.org/paper/a17ace1a85ccb4be885697766d74e6a13fdd5321", "relevance": 3, "abstract": "Large language models (LLMs) can store a significant amount of factual knowledge in their parameters. However, their parametric knowledge may conflict with the information provided in the context. Such conflicts can lead to undesirable model behaviour, such as reliance on outdated or incorrect information. In this work, we investigate whether LLMs can identify knowledge conflicts and whether it is possible to know which source of knowledge the model will rely on by analysing the residual stream of the LLM. Through probing tasks, we find that LLMs can internally register the signal of knowledge conflict in the residual stream, which can be accurately detected by probing the intermediate model activations. This allows us to detect conflicts within the residual stream before generating the answers without modifying the input or model parameters. Moreover, we find that the residual stream shows significantly different patterns when the model relies on contextual knowledge versus parametric knowledge to resolve conflicts. This pattern can be employed to estimate the behaviour of LLMs when conflict happens and prevent unexpected answers before producing the answers. Our analysis offers insights into how LLMs internally manage knowledge conflicts and provides a foundation for developing methods to control the knowledge selection processes.", "citations": 8}
{"title": "Quiet Feature Learning in Algorithmic Tasks", "year": 2025, "authors": "Prudhviraj Naidu, Zixian Wang, Leon Bergen, R. Paturi", "url": "https://www.semanticscholar.org/paper/eb289b4010ef4fd2f3bf6e6375363ab86d69338d", "relevance": 3, "abstract": "We train Transformer-based language models on ten foundational algorithmic tasks and observe pronounced phase transitions in their loss curves that deviate from established power-law scaling trends. Over large ranges of compute, the validation loss barely improves, then abruptly decreases. Probing the models'internal representations reveals that quiet features are learned prior to any decrease in task loss. These quiet features represent intermediate algorithmic computations that do not by themselves improve the output loss. Ablation experiments demonstrate that individual quiet features are causally necessary for task performance. Our results demonstrate that substantial representational progress can remain hidden beneath an apparently flat loss curve, challenging the prevailing use of cross-entropy as a proxy for learning and motivating richer diagnostics for monitoring model training.", "citations": 0}
{"title": "Training Language Models to Explain Their Own Computations", "year": 2025, "authors": "Belinda Z. Li, Zifan Carl Guo, Vincent Huang, Jacob Steinhardt, Jacob Andreas", "url": "https://www.semanticscholar.org/paper/2f967d2b86217368a36511d082ae465de04980c2", "relevance": 3, "abstract": "Can language models (LMs) learn to faithfully describe their internal computations? Are they better able to describe themselves than other models? We study the extent to which LMs'privileged access to their own internals can be leveraged to produce new techniques for explaining their behavior. Using existing interpretability techniques as a source of ground truth, we fine-tune LMs to generate natural language descriptions of (1) the information encoded by LM features, (2) the causal structure of LMs'internal activations, and (3) the influence of specific input tokens on LM outputs. When trained with only tens of thousands of example explanations, explainer models exhibit non-trivial generalization to new queries. This generalization appears partly attributable to explainer models'privileged access to their own internals: using a model to explain its own computations generally works better than using a *different* model to explain its computations (even if the explainer model is significantly more capable than the target). Our results suggest not only that LMs can learn to reliably explain their internal computations, but that such explanations offer a scalable complement to existing interpretability methods. Code and data at https://github.com/TransluceAI/introspective-interp", "citations": 7}
{"title": "SAGE: Scalable Ground Truth Evaluations for Large Sparse Autoencoders", "year": 2024, "authors": "Constantin Venhoff, Anisoara Calinescu, Philip Torr, Christian Schr\u00f6der de Witt", "url": "https://www.semanticscholar.org/paper/70d8974eeeedb81527a504789bb1daa5cef34d41", "relevance": 3, "abstract": "A key challenge in interpretability is to decompose model activations into meaningful features. Sparse autoencoders (SAEs) have emerged as a promising tool for this task. However, a central problem in evaluating the quality of SAEs is the absence of ground truth features to serve as an evaluation gold standard. Current evaluation methods for SAEs are therefore confronted with a significant trade-off: SAEs can either leverage toy models or other proxies with predefined ground truth features; or they use extensive prior knowledge of realistic task circuits. The former limits the generalizability of the evaluation results, while the latter limits the range of models and tasks that can be used for evaluations. We introduce SAGE: Scalable Autoencoder Ground-truth Evaluation, a ground truth evaluation framework for SAEs that scales to large state-of-the-art SAEs and models. We demonstrate that our method can automatically identify task-specific activations and compute ground truth features at these points. Compared to previous methods we reduce the training overhead by introducing a novel reconstruction method that allows to apply residual stream SAEs to sublayer activations. This eliminates the need for SAEs trained on every task-specific activation location. Then we validate the scalability of our framework, by evaluating SAEs on novel tasks on Pythia70M, GPT-2 Small, and Gemma-2-2. Our framework therefore paves the way for generalizable, large-scale evaluations of SAEs in interpretability research.", "citations": 1}
{"title": "Characterizing stable regions in the residual stream of LLMs", "year": 2024, "authors": "Jett Janiak, Jacek Karwowski, Chatrik Singh Mangat, Giorgi Giglemiani, Nora Petrova, Stefan Heimersheim", "url": "https://api.semanticscholar.org/CorpusId:272880695", "relevance": 3, "abstract": "We identify stable regions in the residual stream of Transformers, where the model's output remains insensitive to small activation changes, but exhibits high sensitivity at region boundaries. These regions emerge during training and become more defined as training progresses or model size increases. The regions appear to be much larger than previously studied polytopes. Our analysis suggests that these stable regions align with semantic distinctions, where similar prompts cluster within regions, and activations from the same region lead to similar next token predictions. This work provides a promising research direction for understanding the complexity of neural networks, shedding light on training dynamics, and advancing interpretability.", "citations": 2}
{"title": "Steering LLM Reasoning Through Bias-Only Adaptation", "year": 2025, "authors": "Viacheslav Sinii, Alexey Gorbatovski, Artem Cherepanov, Boris Shaposhnikov, Nikita Balagansky, Daniil Gavrilov", "url": "https://www.semanticscholar.org/paper/ca3ad95b0dd34095473aa2417232b903fdedfe32", "relevance": 3, "abstract": "We show that training a single $d$-dimensional steering vector per layer with reinforcement learning, while freezing all base weights, matches the accuracy of fully RL-tuned reasoning models on mathematical-reasoning tasks. On an 8 billion-parameter model this adds only $\\approx 0.0016\\%$ additional parameters and reproduces performance across a range of base models and mathematical-reasoning benchmarks. These results tighten the upper bound on the parameter budget required for high-level chain-of-thought reasoning, indicating that millions of adapter weights are unnecessary. The minimal trainable footprint reduces optimizer memory and inter-GPU communication, lowering the overall cost of fine-tuning. Moreover, a logit-lens analysis shows that the learned vectors amplify coherent token directions, providing clearer insight into the model's internal computations.", "citations": 4}
{"title": "You Do Not Fully Utilize Transformer's Representation Capacity", "year": 2025, "authors": "Gleb Gerasimov, Yaroslav Aksenov, Nikita Balagansky, Viacheslav Sinii, Daniil Gavrilov", "url": "https://api.semanticscholar.org/CorpusId:276317819", "relevance": 3, "abstract": "In contrast to RNNs, which compress their history into a single hidden state, Transformers can attend to all past tokens directly. However, standard Transformers rely solely on the hidden state from the previous layer to represent the entire context. We show that this design choice induces representation collapse and degrades performance. To address this issue, we introduce Layer-Integrated Memory (LIMe), a lightweight extension that leverages existing key-value buffers and learns per-head, per-layer routing weights to integrate representations from all previous layers with negligible overhead. Through extensive experiments-including language modeling, synthetic reasoning benchmarks, and very deep architectures-LIMe consistently achieves faster convergence, lower perplexity per FLOP, and substantial accuracy improvements on synthetic tasks while preserving higher value-vector entropy and improved token separability. Finally, our analysis of the learned routing weights reveals systematic reuse of both local and long-distance features, demonstrating how LIMe mitigates collapse, unlocks richer representations without increasing hidden-state size, and points to promising directions for future research.", "citations": 1}
{"title": "Forbidden Facts: An Investigation of Competing Objectives in Llama-2", "year": 2023, "authors": "Tony T. Wang, Miles Wang, Kaivu Hariharan, N. Shavit", "url": "https://www.semanticscholar.org/paper/a8f0207dfae566bdc798a18198d1c914a5e0aed7", "relevance": 3, "abstract": "LLMs often face competing pressures (for example helpfulness vs. harmlessness). To understand how models resolve such conflicts, we study Llama-2-chat models on the forbidden fact task. Specifically, we instruct Llama-2 to truthfully complete a factual recall statement while forbidding it from saying the correct answer. This often makes the model give incorrect answers. We decompose Llama-2 into 1000+ components, and rank each one with respect to how useful it is for forbidding the correct answer. We find that in aggregate, around 35 components are enough to reliably implement the full suppression behavior. However, these components are fairly heterogeneous and many operate using faulty heuristics. We discover that one of these heuristics can be exploited via a manually designed adversarial attack which we call The California Attack. Our results highlight some roadblocks standing in the way of being able to successfully interpret advanced ML systems. Project website available at https://forbiddenfacts.github.io .", "citations": 3}
{"title": "Reasoning-Finetuning Repurposes Latent Representations in Base Models", "year": 2025, "authors": "Jake Ward, Chuqiao Lin, Constantin Venhoff, Neel Nanda", "url": "https://www.semanticscholar.org/paper/97a46b62b2dc80c35e250ab22055bc66fdce158c", "relevance": 3, "abstract": "Backtracking, an emergent behavior elicited by reasoning fine-tuning, has been shown to be a key mechanism in reasoning models'enhanced capabilities. Prior work has succeeded in manipulating this behavior via steering vectors, but the underlying mechanism remains poorly understood. In this work, we show that the emergence of backtracking in DeepSeek-R1-Distill-Llama-8B is in part driven by a repurposed direction already present in base model activations. Specifically, we identify a direction in base Llama-3.1-8B's residual stream which systematically induces backtracking when used to steer the distilled reasoning model, and find that the effects of steering with this direction cannot be trivially explained by token-level attributes. We further find that this direction does not induce backtracking in the base model, suggesting that the reasoning finetuning process repurposes pre-existing representations to form new behavioral circuits. Additionally, we hypothesize that this direction is one of several which may work together to mediate backtracking. Our findings offer a compelling picture that reasoning-finetuned models repurpose pre-existing base model representations, rather than learn new capabilities from scratch.", "citations": 10}
{"title": "Simulated Adoption: Decoupling Magnitude and Direction in LLM In-Context Conflict Resolution", "year": 2026, "authors": "Long Zhang, Fangwei Lin", "url": "https://www.semanticscholar.org/paper/605ff7231a81c3424d51ae9b9d8f1d21f017cc5d", "relevance": 3, "abstract": "Large Language Models (LLMs) frequently prioritize conflicting in-context information over pre-existing parametric memory, a phenomenon often termed sycophancy or compliance. However, the mechanistic realization of this behavior remains obscure, specifically how the model resolves these knowledge conflicts through compliance, and whether this suppression arises from signal magnitude dilution or directional geometric alteration within the residual stream. To resolve this, we conducted a layer-wise geometric analysis across Qwen-3-4B, Llama-3.1-8B, and GLM-4-9B, decomposing the residual stream updates induced by counter-factual contexts into radial (norm-based) and angular (cosine-based) components. Our empirical results reject the universality of the\"Manifold Dilution\"hypothesis, as two of the three architectures maintained stable residual norms despite exhibiting significant performance degradation on factual queries. Instead, we observed that compliance is consistently characterized by\"Orthogonal Interference,\"where the conflicting context injects a steering vector that is quasi-orthogonal to the ground-truth direction, effectively rotating the hidden state representation. This suggests that models do not\"unlearn\"or suppress the magnitude of internal truths but rather employ a mechanism of geometric displacement to bypass the correct unembedding vector, effectively simulating adoption while preserving the original structural magnitude. These findings challenge scalar confidence metrics for detecting hallucinations and underscore the necessity of vectorial monitoring to distinguish between genuine knowledge integration and superficial in-context mimicry.", "citations": 0}
{"title": "Learning a Generative Meta-Model of LLM Activations", "year": 2026, "authors": "Grace Luo, Jiahai Feng, Trevor Darrell, Alec Radford, Jacob Steinhardt", "url": "https://www.semanticscholar.org/paper/f00ee65566884274091b003370fe291c6f959d03", "relevance": 3, "abstract": "Existing approaches for analyzing neural network activations, such as PCA and sparse autoencoders, rely on strong structural assumptions. Generative models offer an alternative: they can uncover structure without such assumptions and act as priors that improve intervention fidelity. We explore this direction by training diffusion models on one billion residual stream activations, creating\"meta-models\"that learn the distribution of a network's internal states. We find that diffusion loss decreases smoothly with compute and reliably predicts downstream utility. In particular, applying the meta-model's learned prior to steering interventions improves fluency, with larger gains as loss decreases. Moreover, the meta-model's neurons increasingly isolate concepts into individual units, with sparse probing scores that scale as loss decreases. These results suggest generative meta-models offer a scalable path toward interpretability without restrictive structural assumptions. Project page: https://generative-latent-prior.github.io.", "citations": 0}
{"title": "Beyond I'm Sorry, I Can't: Dissecting Large Language Model Refusal", "year": 2025, "authors": "Nirmalendu Prakash, Yeo Wei Jie, Amirali Abdullah, Ranjan Satapathy, Erik Cambria, Roy Ka-Wei Lee", "url": "https://www.semanticscholar.org/paper/ec4e56b6fdf1f87272440b0790aaf6e705e8b4b4", "relevance": 3, "abstract": "Refusal on harmful prompts is a key safety behaviour in instruction-tuned large language models (LLMs), yet the internal causes of this behaviour remain poorly understood. We study two public instruction-tuned models, Gemma-2-2B-IT and LLaMA-3.1-8B-IT, using sparse autoencoders (SAEs) trained on residual-stream activations. Given a harmful prompt, we search the SAE latent space for feature sets whose ablation flips the model from refusal to compliance, demonstrating causal influence and creating a jailbreak. Our search proceeds in three stages: (1) Refusal Direction: find a refusal-mediating direction and collect SAE features near that direction; (2) Greedy Filtering: prune to a minimal set; and (3) Interaction Discovery: fit a factorization machine (FM) that captures nonlinear interactions among the remaining active features and the minimal set. This pipeline yields a broad set of jailbreak-critical features, offering insight into the mechanistic basis of refusal. Moreover, we find evidence of redundant features that remain dormant unless earlier features are suppressed. Our findings highlight the potential for fine-grained auditing and targeted intervention in safety behaviours by manipulating the interpretable latent space.", "citations": 4}
{"title": "Attention is All you Need", "year": 2017, "authors": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, I. Polosukhin", "url": "https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776", "relevance": 1, "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.", "citations": 165529}
{"title": "Evaluating and Designing Sparse Autoencoders by Approximating Quasi-Orthogonality", "year": 2025, "authors": "Sewoong Lee, Adam Davies, Marc E. Canby, J. Hockenmaier", "url": "https://api.semanticscholar.org/CorpusId:277468308", "relevance": 1, "abstract": "Sparse autoencoders (SAEs) are widely used in mechanistic interpretability research for large language models; however, the state-of-the-art method of using $k$-sparse autoencoders lacks a theoretical grounding for selecting the hyperparameter $k$ that represents the number of nonzero activations, often denoted by $\\ell_0$. In this paper, we reveal a theoretical link that the $\\ell_2$-norm of the sparse feature vector can be approximated with the $\\ell_2$-norm of the dense vector with a closed-form error, which allows sparse autoencoders to be trained without the need to manually determine $\\ell_0$. Specifically, we validate two applications of our theoretical findings. First, we introduce a new methodology that can assess the feature activations of pre-trained SAEs by computing the theoretically expected value from the input embedding, which has been overlooked by existing SAE evaluation methods and loss functions. Second, we introduce a novel activation function, top-AFA, which builds upon our formulation of approximate feature activation (AFA). This function enables top-$k$ style activation without requiring a constant hyperparameter $k$ to be tuned, dynamically determining the number of activated features for each input. By training SAEs on three intermediate layers to reconstruct GPT2 hidden embeddings for over 80 million tokens from the OpenWebText dataset, we demonstrate the empirical merits of this approach and compare it with current state-of-the-art $k$-sparse autoencoders. Our code is available at: https://github.com/SewoongLee/top-afa-sae.", "citations": 2}
{"title": "AI-PCC based Streaming Framework using Residual Parameters of Neural Networks", "year": 2023, "authors": "Minseok Lee, Junsik Kim, Seongbae Rhee, Kyuheon Kim", "url": "https://www.semanticscholar.org/paper/2c01c0bb6756071f5be2d653a119579b1151d070", "relevance": 1, "abstract": "", "citations": 0}
{"title": "On the Implementation of Residual Knowledge Continuous Assessment Technology in an Educational Organization Using Artificial Intelligence Tools", "year": 2024, "authors": "M. Buinevich, Alexey Shkerin, Tatyana Smolentseva, Maria Puchkova", "url": "https://www.semanticscholar.org/paper/27d3d37fcc0d84ab831249e2902f0106b1b16749", "relevance": 1, "abstract": "The integration of artificial intelligence (AI) into the educational process represents a paradigm shift towards an adaptive and responsive higher education system. This paper examines an example of using AI to analyze the results of the technology for continuous assessment of residual knowledge (CARK), in the case of streaming disciplines. The problem of the complexity of technical implementation at the stage of assessing student responses in free form for residual knowledge assessment is formulated. The issue of computational power when training AI models for the tasks mentioned in the paper is considered.", "citations": 1}
{"title": "Mixed graph convolution and residual transformation network for skeleton-based action recognition", "year": 2021, "authors": "Shuhua Liu, Xiaoying Bai, Ming Fang, Lanting Li, C. Hung", "url": "https://www.semanticscholar.org/paper/67f7b810e8968f9ee7640498af9b936335d310f8", "relevance": 1, "abstract": "", "citations": 34}
{"title": "Multi-Direction Networks With Attentional Spectral Prior for Hyperspectral Image Classification", "year": 2021, "authors": "Bobo Xi, Jiaojiao Li, Yunsong Li, Rui Song, Yu-Bai Xiao, Yanzi Shi, Q. Du", "url": "https://www.semanticscholar.org/paper/45865712fbe041397ae58c9e1ad6cdc74cadca34", "relevance": 1, "abstract": "Convolutional neural networks (CNNs) have achieved prominent progress in recent years and demonstrated remarkable properties in spectral\u2013spatial hyperspectral image (HSI) classification. However, conventional spatial-context-based CNNs commonly adopt the single patchwise scheme to represent the to-be-classified samples, which often fails to completely investigate the wealthy spectral\u2013spatial information in complicated situations. For instance, it has great probability to cause misclassifications on the irregular or inhomogeneous areas, especially for the borders across different classes. To counteract this deficiency, we propose a unified multi-direction network (MDN) for HSI Classification (HSIC), which can exhaustively explore the abundant spectral and detailed spatial-context information through multi-direction samples. Additionally, considering the image-spectrum merged structure of the HSI, 3-D Squeeze-and-Excitation residual (3DSERes) blocks are devised in each stream of the framework to consecutively learn the spectral and spatial from low-level to high-level features. Specifically, 3DSERes can not only facilitate fluent gradient in backpropagation through skip connections, but also emphasize the significant spectral\u2013spatial features and constrain the futile ones. This characteristic is beneficial to enhance the model\u2019s generalization capability even with limited training samples. Furthermore, for properly aggregating the multi-direction deep features, we exploit the simple, yet effective attentional spectral prior (ASP) creatively through leveraging the original spectral correlations. Extensive experimental results on three benchmark data sets indicate that the proposed MDN-ASP can achieve promising classification performance compared to the state-of-the-art methods.", "citations": 27}
{"title": "MUDDFormer: Breaking Residual Bottlenecks in Transformers via Multiway Dynamic Dense Connections", "year": 2025, "authors": "Da Xiao, Qingye Meng, Shengping Li, Xingyuan Yuan", "url": "https://www.semanticscholar.org/paper/eec6b87ade0f50555d2639317b83d39e1210fed4", "relevance": 1, "abstract": "We propose MUltiway Dynamic Dense (MUDD) connections, a simple yet effective method to address the limitations of residual connections and enhance cross-layer information flow in Transformers. Unlike existing dense connection approaches with static and shared connection weights, MUDD generates connection weights dynamically depending on hidden states at each sequence position and for each decoupled input stream (the query, key, value or residual) of a Transformer block. MUDD connections can be seamlessly integrated into any Transformer architecture to create MUDDFormer. Extensive experiments show that MUDDFormer significantly outperforms Transformers across various model architectures and scales in language modeling, achieving the performance of Transformers trained with 1.8X-2.4X compute. Notably, MUDDPythia-2.8B matches Pythia-6.9B in pretraining ppl and downstream tasks and even rivals Pythia-12B in five-shot settings, while adding only 0.23% parameters and 0.4% computation. Code in JAX and PyTorch and pre-trained models are available at https://github.com/Caiyun-AI/MUDDFormer .", "citations": 11}
{"title": "Enhancing Road Safety with AI-Powered System for Effective Detection and Localization of Emergency Vehicles by Sound", "year": 2025, "authors": "Lucas Banchero, Francisco Vacalebri-Lloret, J. M. Mossi, Jos\u00e9 J. L\u00f3pez", "url": "https://www.semanticscholar.org/paper/346cfbd1e272bc9c5abcd28c0b5a59bc1980959e", "relevance": 1, "abstract": "This work presents the design and implementation of an emergency sound detection and localization system, specifically for sirens and horns, aimed at enhancing road safety in automotive environments. The system integrates specialized hardware and advanced artificial intelligence algorithms to function effectively in complex acoustic conditions, such as urban traffic and environmental noise. It introduces an aerodynamic structure designed to mitigate wind noise and vibrations in microphones, ensuring high-quality audio capture. In terms of analysis through artificial intelligence, the system utilizes transformer-based architecture and convolutional neural networks (such as residual networks and U-NET) to detect, localize, clean, and analyze nearby sounds. Additionally, it operates in real-time through sliding windows, providing the driver with accurate visual information about the direction, proximity, and trajectory of the emergency sound. Experimental results demonstrate high accuracy in both controlled and real-world conditions, with a detection accuracy of 98.86% for simulated data and 97.5% for real-world measurements, and localization with an average error of 5.12\u00b0 in simulations and 10.30\u00b0 in real-world measurements. These results highlight the effectiveness of the proposed approach for integration into driver assistance systems and its potential to improve road safety.", "citations": 5}
{"title": "Research on the Four Key Principles Governing Enterprise Data Rights in the Age of AI", "year": 2025, "authors": "Purui Liu", "url": "https://www.semanticscholar.org/paper/1df4812223306484a7729990743ed957f0a09928", "relevance": 1, "abstract": "The study proposes a hybrid control architecture for industrial robots that incorporates deep neural networks for adaptive tuning in all operating domains through online dynamic compensation. The system synchronizes six-dimensional force/torque sensor data, combined motor current, and terminal acceleration via an Ether-CAT bus to construct a compensation basis that includes 23 dimensions of real-time state observation. Dynamic residual learning is performed using a two-stream network design, with the primary network processing the historical state window and the secondary network parsing joint-space parameters. The network is trained using a move-learning strategy and pre-trained in a high-fidelity simulation environment of the Franka Emika robot.", "citations": 0}
{"title": "Integration of AI-Driven Electronic Warfare Radio Monitoring Systems with Antenna Array Direction Finding for Advanced Signal Classification and Localization", "year": 2025, "authors": "Mohamed A. Aboelazm", "url": "https://www.semanticscholar.org/paper/9b4e2cb5a77215e6c057eddf5d1f0889ab364b96", "relevance": 1, "abstract": "Electronic warfare (EW) systems are critical for maintaining strategic advantage in modern military operations by detecting, classifying, and neutralizing adversarial signals. This paper presents an integrated framework that merges AI-driven signal processing techniques antenna array-based direction finding (DF) techniques to enhance the capabilities of electronic warfare (EW) systems. The integration of AI with antenna array systems thus not only improves situational awareness but also enhances decision-making in contested environments. The proposed system leverages deep learning models-including convolutional neural networks (CNNs) for signal classification and its developmental version, Residual Neural Networks (ResNet), to extract and analyze complex signal features in varying electromagnetic environments. Coupled with high-resolution direction finding methods such as MUSIC and Sparse-Music, the system provides precise angle-of-arrival (AOA) estimation even under challenging conditions characterized by low signal-to-noise ratios (SNR) and overlapping transmissions. The proposed architecture is validated through simulations and hardware-in-the-loop (HIL) tests, demonstrating superior angular resolution. The results highlight the potential of AI-driven EW systems to operate effectively in dense signal environments with limited prior knowledge. The paper concludes by outlining the practical deployment challenges and potential pathways for future research, including multi-sensor collaboration and adversarial signal resistance. This work represents a significant step toward next-generation cognitive electronic warfare capabilities.", "citations": 0}
{"title": "AI and semantic ontology for personalized activity eCoaching in healthy lifestyle recommendations: a meta-heuristic approach", "year": 2023, "authors": "A. Chatterjee, Nibedita Pahari, Andreas Prinz, M. Riegler", "url": "https://www.semanticscholar.org/paper/344885c59c275217ae2ded046f1264490686a1e5", "relevance": 1, "abstract": "Background Automated coaches (eCoach) can help people lead a healthy lifestyle (e.g., reduction of sedentary bouts) with continuous health status monitoring and personalized recommendation generation with artificial intelligence (AI). Semantic ontology can play a crucial role in knowledge representation, data integration, and information retrieval. Methods This study proposes a semantic ontology model to annotate the AI predictions, forecasting outcomes, and personal preferences to conceptualize a personalized recommendation generation model with a hybrid approach. This study considers a mixed activity projection method that takes individual activity insights from the univariate time-series prediction and ensemble multi-class classification approaches. We have introduced a way to improve the prediction result with a residual error minimization (REM) technique and make it meaningful in recommendation presentation with a Na\u00efve-based interval prediction approach. We have integrated the activity prediction results in an ontology for semantic interpretation. A SPARQL query protocol and RDF Query Language (SPARQL) have generated personalized recommendations in an understandable format. Moreover, we have evaluated the performance of the time-series prediction and classification models against standard metrics on both imbalanced and balanced public PMData and private MOX2-5 activity datasets. We have used Adaptive Synthetic (ADASYN) to generate synthetic data from the minority classes to avoid bias. The activity datasets were collected from healthy adults (n\u2009=\u200916 for public datasets; n\u2009=\u200915 for private datasets). The standard ensemble algorithms have been used to investigate the possibility of classifying daily physical activity levels into the following activity classes: sedentary (0), low active (1), active (2), highly active (3), and rigorous active (4). The daily step count, low physical activity (LPA), medium physical activity (MPA), and vigorous physical activity (VPA) serve as input for the classification models. Subsequently, we re-verify the classifiers on the private MOX2-5 dataset. The performance of the ontology has been assessed with reasoning and SPARQL query execution time. Additionally, we have verified our ontology for effective recommendation generation. Results We have tested several standard AI algorithms and selected the best-performing model with optimized configuration for our use case by empirical testing. We have found that the autoregression model with the REM method outperforms the autoregression model without the REM method for both datasets. Gradient Boost (GB) classifier outperforms other classifiers with a mean accuracy score of 98.00%, and 99.00% for imbalanced PMData and MOX2-5 datasets, respectively, and 98.30%, and 99.80% for balanced PMData and MOX2-5 datasets, respectively. Hermit reasoner performs better than other ontology reasoners under defined settings. Our proposed algorithm shows a direction to combine the AI prediction forecasting results in an ontology to generate personalized activity recommendations in eCoaching. Conclusion The proposed method combining step-prediction, activity-level classification techniques, and personal preference information with semantic rules is an asset for generating personalized recommendations.", "citations": 9}
{"title": "Development of AI-based Adaptive Compressor Design for Partial Load Efficiency and Residual Energy Utilization", "year": 2025, "authors": "Sandy Suryady, E. Nugroho", "url": "https://www.semanticscholar.org/paper/82a5c2009f364480eacdd39e6d16909fb6780446", "relevance": 1, "abstract": "The growing demand for energy-efficient and intelligent thermal systems has driven significant advancements in adaptive compressor design. This paper presents a comprehensive literature review on the development of AI-based compressor systems, with a specific focus on enhancing efficiency under partial-load conditions and optimizing the utilization of residual energy. Through the synthesis of five recent high-impact studies (2020\u20132025), we examine the application of deep reinforcement learning (DRL), hybrid evolutionary algorithms, and neural network surrogate modeling in compressor optimization. Key findings indicate that model-based DRL combined with surrogate CFD can achieve up to 8% efficiency gains at off-design conditions. Hybrid approaches integrating Genetic Algorithms (GA) with DRL reduce optimization time by 30% while improving pressure ratios. Neural network surrogates provide high-speed, real-time performance predictions with less than 1% error, enabling mass iterative design. Furthermore, intelligent load classification using radial basis function networks (RBFN) allows adaptive response to varying operating conditions with over 95% accuracy. Collectively, these methods form a framework for intelligent, self-optimizing compressor systems capable of real-time adaptation and energy recovery. The results suggest that AI-enhanced adaptive compressors represent a transformative direction for energy-sensitive sectors, including HVAC, power generation, and sustainable\u00a0industry.", "citations": 0}
{"title": "The AI Revolution in Time Series: Challenges and Opportunites", "year": 2025, "authors": "Yan Liu", "url": "https://www.semanticscholar.org/paper/0127d89a19ce00299d165ecfa0281679d73475d1", "relevance": 1, "abstract": "Recent advancements in deep learning and artificial intelligence have driven significant progress in time series modeling and analysis. On one hand, researchers seek breakthroughs in performance on classical tasks such as forecasting, anomaly detection, classification, etc. On the other hand, it is intriguing to explore the potential for answering more complex inference and reasoning tasks from time series. In this keynote, I will examine the pathways toward foundation models for time series and discuss future research directions in this rapidly evolving field. The remarkable success of foundation models in natural language processing - exemplified by Generative Pre-trained Transformers (GPT) - suggests their potential to revolutionize time series analysis. I will introduce our recent efforts along this direction, including TEMPO, a novel framework designed to learn effective time series representations by leveraging two key inductive biases: one is explicit decomposition of trend, seasonal, and residual components, and the second is prompt-based distribution adaptation for diverse time series types. Beyond representation learning, practical applications demands advanced reasoning capabilities with multi-step time series inference task, requiring both compositional reasoning and computational precision. To tackle this challenge, I will discuss TS-reasoner, a program-aided inference agent that integrates large language models (LLMs) with structured execution pipelines, in-context learning, and self-correction mechanisms. I will discuss a new benchmark dataset and evaluation framework to systematically assess multi-step time series reasoning. By bridging deep learning advances with structured reasoning, I will highlight the next frontier in time series research, i.e., developing foundation models that enhance forecasting performance, generative models, and reasoning capabilities from time series across diverse applications.", "citations": 0}
{"title": "Causal Fingerprints of AI Generative Models", "year": 2025, "authors": "Huisha Xu, Chi Liu, Congcong Zhu, Minghao Wang, Youyang Qu, Longxiang Gao", "url": "https://www.semanticscholar.org/paper/c3309801faab0672362da7a659dcb2f83d7e2673", "relevance": 1, "abstract": "AI generative models leave implicit traces in their generated images, which are commonly referred to as model fingerprints and are exploited for source attribution. Prior methods rely on model-specific cues or synthesis artifacts, yielding limited fingerprints that may generalize poorly across different generative models. We argue that a complete model fingerprint should reflect the causality between image provenance and model traces, a direction largely unexplored. To this end, we conceptualize the \\emph{causal fingerprint} of generative models, and propose a causality-decoupling framework that disentangles it from image-specific content and style in a semantic-invariant latent space derived from pre-trained diffusion reconstruction residual. We further enhance fingerprint granularity with diverse feature representations. We validate causality by assessing attribution performance across representative GANs and diffusion models and by achieving source anonymization using counterfactual examples generated from causal fingerprints. Experiments show our approach outperforms existing methods in model attribution, indicating strong potential for forgery detection, model copyright tracing, and identity protection.", "citations": 0}
{"title": "Residual learning-based two-stream network for RGB-T object tracking", "year": 2022, "authors": "Yili Chen, Minjie Wan, Yunkai Xu, Xiaojie Zhang, Qian Chen, G. Gu", "url": "https://www.semanticscholar.org/paper/8fc89b11842c1b896f94f785d6b50040e0cab767", "relevance": 1, "abstract": "Abstract. RGB-T object tracking is a branch of visual tracking that has been widely applied to many fields, such as intelligent transportation and urban monitoring. Due to the interference of background clutter and occlusion, the existing trackers still suffer from the problems of unreasonable modal fusion strategy, insufficient feature extraction, and loss of semantic information. To solve these problems, we propose a residual learning-based two stream network for RGB-T object tracking. The overall feature extraction network is composed of three branches, and multi-layer convolutions are utilized to extract the features of visible, thermal, and fused images, respectively. First, aiming at improving the effectiveness of feature extraction, a weight generation module for hierarchical feature weight calculation is designed to guide the direction of feature fusion. Then, the residual block is employed to replace the single-layer convolution in order to increase the depth of the network, by which deeper semantic features are learned and the loss of semantic information is alleviated. Finally, a loss function with a penalty term is developed to adjust our network toward the direction of the best tracking performance of the dual modalities. This overcomes the negative impact of poor mode on model training. Experiments implemented on public RGB-T datasets indicate that our algorithm outperforms the recent state-of-the-art trackers in terms of both precision rate and success rate. Compared with the second-best comparison algorithm, our tracker improves the above two metrics by 0.4% and 1.5%, respectively. Our codes are available at https://github.com/MinjieWan/Residual-learning-based-two-stream-network-for-RGB-T-object-tracking.", "citations": 1}
