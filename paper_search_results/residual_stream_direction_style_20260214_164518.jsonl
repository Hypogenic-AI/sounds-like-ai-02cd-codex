{"title": "Toward Preference-aligned Large Language Models via Residual-based Model Steering", "year": 2025, "authors": "Lucio La Cava, Andrea Tagarelli", "url": "https://api.semanticscholar.org/CorpusId:281675737", "relevance": 3, "abstract": "Preference alignment is a critical step in making Large Language Models (LLMs) useful and aligned with (human) preferences. Existing approaches such as Reinforcement Learning from Human Feedback or Direct Preference Optimization typically require curated data and expensive optimization over billions of parameters, and eventually lead to persistent task-specific models. In this work, we introduce Preference alignment of Large Language Models via Residual Steering (PaLRS), a training-free method that exploits preference signals encoded in the residual streams of LLMs. From as few as one hundred preference pairs, PaLRS extracts lightweight, plug-and-play steering vectors that can be applied at inference time to push models toward preferred behaviors. We evaluate PaLRS on various small-to-medium-scale open-source LLMs, showing that PaLRS-aligned models achieve consistent gains on mathematical reasoning and code generation benchmarks while preserving baseline general-purpose performance. Moreover, when compared to DPO-aligned models, they perform better with huge time savings. Our findings highlight that PaLRS offers an effective, much more efficient and flexible alternative to standard preference optimization pipelines, offering a training-free, plug-and-play mechanism for alignment with minimal data.", "citations": 0}
{"title": "Effectively Steer LLM To Follow Preference via Building Confident Directions", "year": 2025, "authors": "Bingqing Song, Boran Han, Shuai Zhang, Hao Wang, Haoyang Fang, Bonan Min, Yuyang Wang, Mingyi Hong", "url": "https://api.semanticscholar.org/CorpusId:276782038", "relevance": 3, "abstract": "Having an LLM that aligns with human preferences is essential for accommodating individual needs, such as maintaining writing style or generating specific topics of interest. The majority of current alignment methods rely on fine-tuning or prompting, which can be either costly or difficult to control. Model steering algorithms, which modify the model output by constructing specific steering directions, are typically easy to implement and optimization-free. However, their capabilities are typically limited to steering the model into one of the two directions (i.e., bidirectional steering), and there has been no theoretical understanding to guarantee their performance. In this work, we propose a theoretical framework to understand and quantify the model steering methods. Inspired by the framework, we propose a confident direction steering method (CONFST) that steers LLMs via modifying their activations at inference time. More specifically, CONFST builds a confident direction that is closely aligned with users' preferences, and this direction is then added to the activations of the LLMs to effectively steer the model output. Our approach offers three key advantages over popular bidirectional model steering methods: 1) It is more powerful, since multiple (i.e. more than two) users' preferences can be aligned simultaneously; 2) It is simple to implement, since there is no need to determine which layer to add the steering vector to; 3) No explicit user instruction is required. We validate our method on GPT-2 XL (1.5B), Mistral (7B) and Gemma-it (9B) models for tasks that require shifting the output of LLMs across various topics and styles, achieving superior performance over competing methods.", "citations": 8}
{"title": "Fine-Grained control over Music Generation with Activation Steering", "year": 2025, "authors": "Dipanshu Panda, Jayden Koshy Joe, R. HarshithM, Swathi Narashiman, Pranay Mathur, Anish Veerakumar, Aniruddh Krishna, A. Keerthiharan", "url": "https://api.semanticscholar.org/CorpusId:279318720", "relevance": 3, "abstract": "We present a method for fine-grained control over music generation through inference-time interventions on an autoregressive generative music transformer called MusicGen. Our approach enables timbre transfer, style transfer, and genre fusion by steering the residual stream using weights of linear probes trained on it, or by steering the attention layer activations in a similar manner. We observe that modelling this as a regression task provides improved performance, hypothesizing that the mean-squared-error better preserve meaningful directional information in the activation space. Combined with the global conditioning offered by text prompts in MusicGen, our method provides both global and local control over music generation. Audio samples illustrating our method are available at our demo page.", "citations": 0}
{"title": "Steering Language Models With Activation Engineering", "year": 2023, "authors": "Alexander Matt Turner, Lisa Thiergart, Gavin Leech, David S. Udell, Juan J. Vazquez, Ulisse Mini, M. MacDiarmid", "url": "https://api.semanticscholar.org/CorpusId:261049449", "relevance": 3, "abstract": "Prompt engineering and finetuning aim to maximize language model performance on a given metric (like toxicity reduction). However, these methods do not fully elicit a model's capabilities. To reduce this gap, we introduce activation engineering: the inference-time modification of activations in order to control (or steer) model outputs. Specifically, we introduce the Activation Addition (ActAdd) technique, which contrasts the intermediate activations on prompt pairs (such as\"Love\"versus\"Hate\") to compute a steering vector (Subramani et al. 2022). By tactically adding in e.g. the\"Love\"-\"Hate\"steering vector during the forward pass, we achieve SOTA on negative-to-positive sentiment shift and detoxification using models including LLaMA-3 and OPT. ActAdd yields inference-time control over high-level output properties (like topic and sentiment) while preserving performance on off-target tasks. ActAdd is lightweight: it does not require any machine optimization and works with a single pair of data points, which enables rapid iteration over steering. ActAdd demonstrates the power of activation engineering.", "citations": 350}
{"title": "Coinductive guide to inductive transformer heads", "year": 2023, "authors": "Adam Nemecek", "url": "https://api.semanticscholar.org/CorpusId:256597964", "relevance": 3, "abstract": "We argue that all building blocks of transformer models can be expressed with a single concept: combinatorial Hopf algebra. Transformer learning emerges as a result of the subtle interplay between the algebraic and coalgebraic operations of the combinatorial Hopf algebra. Viewed through this lens, the transformer model becomes a linear time-invariant system where the attention mechanism computes a generalized convolution transform and the residual stream serves as a unit impulse. Attention-only transformers then learn by enforcing an invariant between these two paths. We call this invariant Hopf coherence. Due to this, with a degree of poetic license, one could call combinatorial Hopf algebras\"tensors with a built-in loss function gradient\". This loss function gradient occurs within the single layers and no backward pass is needed. This is in contrast to automatic differentiation which happens across the whole graph and needs a explicit backward pass. This property is the result of the fact that combinatorial Hopf algebras have the surprising property of calculating eigenvalues by repeated squaring.", "citations": 0}
{"title": "No Training Wheels: Steering Vectors for Bias Correction at Inference Time", "year": 2025, "authors": "Aviral Gupta, Armaan Sethi, Ameesh Sethi", "url": "https://api.semanticscholar.org/CorpusId:279999303", "relevance": 3, "abstract": "Neural network classifiers trained on datasets with uneven group representation often inherit class biases and learn spurious correlations. These models may perform well on average but consistently fail on atypical groups. For example, in hair color classification, datasets may over-represent females with blond hair, reinforcing stereotypes. Although various algorithmic and data-centric methods have been proposed to address such biases, they often require retraining or significant compute. In this work, we propose a cheap, training-free method inspired by steering vectors used to edit behaviors in large language models. We compute the difference in mean activations between majority and minority groups to define a\"bias vector,\"which we subtract from the model's residual stream. This leads to reduced classification bias and improved worst-group accuracy. We explore multiple strategies for extracting and applying these vectors in transformer-like classifiers, showing that steering vectors, traditionally used in generative models, can also be effective in classification. More broadly, we showcase an extremely cheap, inference time, training free method to mitigate bias in classification models.", "citations": 1}
{"title": "Refusal in Language Models Is Mediated by a Single Direction", "year": 2024, "authors": "Andy Arditi, Oscar Obeso, Aaquib Syed, Daniel Paleka, Nina Rimsky, Wes Gurnee, Neel Nanda", "url": "https://api.semanticscholar.org/CorpusId:270560489", "relevance": 3, "abstract": "Conversational large language models are fine-tuned for both instruction-following and safety, resulting in models that obey benign requests but refuse harmful ones. While this refusal behavior is widespread across chat models, its underlying mechanisms remain poorly understood. In this work, we show that refusal is mediated by a one-dimensional subspace, across 13 popular open-source chat models up to 72B parameters in size. Specifically, for each model, we find a single direction such that erasing this direction from the model's residual stream activations prevents it from refusing harmful instructions, while adding this direction elicits refusal on even harmless instructions. Leveraging this insight, we propose a novel white-box jailbreak method that surgically disables refusal with minimal effect on other capabilities. Finally, we mechanistically analyze how adversarial suffixes suppress propagation of the refusal-mediating direction. Our findings underscore the brittleness of current safety fine-tuning methods. More broadly, our work showcases how an understanding of model internals can be leveraged to develop practical methods for controlling model behavior.", "citations": 465}
{"title": "An Adversarial Example for Direct Logit Attribution: Memory Management in GELU-4L", "year": 2023, "authors": "James Dao, Yeu-Tong Lau, Can Rager, Jett Janiak", "url": "https://api.semanticscholar.org/CorpusId:263834728", "relevance": 3, "abstract": "Prior work suggests that language models manage the limited bandwidth of the residual stream through a \u201cmemory management\u201d mechanism, where certain attention heads and MLP layers clear residual stream directions set by earlier layers. Our study provides concrete evidence for this erasure phenomenon in a 4-layer transformer, identifying heads that consistently remove the output of earlier heads. We further demonstrate that direct logit attribution (DLA), a common technique for interpreting the output of intermediate transformer layers, can show misleading results by not accounting for erasure.", "citations": 5}
{"title": "Revisiting Residual Connections: Orthogonal Updates for Stable and Efficient Deep Networks", "year": 2025, "authors": "Giyeong Oh, Woohyun Cho, Siyeol Kim, Suhwan Choi, Younjae Yu", "url": "https://api.semanticscholar.org/CorpusId:278741008", "relevance": 3, "abstract": "Residual connections are pivotal for deep neural networks, enabling greater depth by mitigating vanishing gradients. However, in standard residual updates, the module's output is directly added to the input stream. This can lead to updates that predominantly reinforce or modulate the existing stream direction, potentially underutilizing the module's capacity for learning entirely novel features. In this work, we introduce Orthogonal Residual Update: we decompose the module's output relative to the input stream and add only the component orthogonal to this stream. This design aims to guide modules to contribute primarily new representational directions, fostering richer feature learning while promoting more efficient training. We demonstrate that our orthogonal update strategy improves generalization accuracy and training stability across diverse architectures (ResNetV2, Vision Transformers) and datasets (CIFARs, TinyImageNet, ImageNet-1k), achieving, for instance, a +3.78 pp top-1 accuracy gain for ViT-B on ImageNet-1k.", "citations": 0}
{"title": "Activation Transport Operators", "year": 2025, "authors": "Andrzej Szablewski, Marek Masiak", "url": "https://api.semanticscholar.org/CorpusId:280711348", "relevance": 3, "abstract": "The residual stream mediates communication between transformer decoder layers via linear reads and writes of non-linear computations. While sparse-dictionary learning-based methods locate features in the residual stream, and activation patching methods discover circuits within the model, the mechanism by which features flow through the residual stream remains understudied. Understanding this dynamic can better inform jailbreaking protections, enable early detection of model mistakes, and their correction. In this work, we propose Activation Transport Operators (ATO), linear maps from upstream to downstream residuals $k$ layers later, evaluated in feature space using downstream SAE decoder projections. We empirically demonstrate that these operators can determine whether a feature has been linearly transported from a previous layer or synthesised from non-linear layer computation. We develop the notion of transport efficiency, for which we provide an upper bound, and use it to estimate the size of the residual stream subspace that corresponds to linear transport. We empirically demonstrate the linear transport, report transport efficiency and the size of the residual stream's subspace involved in linear transport. This compute-light (no finetuning,<50 GPU-h) method offers practical tools for safety, debugging, and a clearer picture of where computation in LLMs behaves linearly.", "citations": 0}
{"title": "Interpreting the Residual Stream of ResNet18", "year": 2024, "authors": "Andr\u00e9 Longon", "url": "https://api.semanticscholar.org/CorpusId:271050492", "relevance": 3, "abstract": "A mechanistic understanding of the computations learned by deep neural networks (DNNs) is far from complete. In the domain of visual object recognition, prior research has illuminated inner workings of InceptionV1, but DNNs with different architectures have remained largely unexplored. This work investigates ResNet18 with a particular focus on its residual stream, an architectural mechanism which InceptionV1 lacks. We observe that for a given block, channel features of the stream are updated along a spectrum: either the input feature skips to the output, the block feature overwrites the output, or the output is some mixture between the input and block features. Furthermore, we show that many residual stream channels compute scale invariant representations through a mixture of the input's smaller-scale feature with the block's larger-scale feature. This not only mounts evidence for the universality of scale equivariance, but also presents how the residual stream further implements scale invariance. Collectively, our results begin an interpretation of the residual stream in visual object recognition, finding it to be a flexible feature manager and a medium to build scale invariant representations.", "citations": 4}
{"title": "Curved Inference: Concern-Sensitive Geometry in Large Language Model Residual Streams", "year": 2025, "authors": "Rob Manson", "url": "https://api.semanticscholar.org/CorpusId:280337467", "relevance": 3, "abstract": "We propose Curved Inference - a geometric Interpretability framework that tracks how the residual stream trajectory of a large language model bends in response to shifts in semantic concern. Across 20 matched prompts spanning emotional, moral, perspective, logical, identity, environmental, and nonsense domains, we analyse Gemma3-1b and LLaMA3.2-3b using five native-space metrics, with a primary focus on curvature (\\k{appa}_i) and salience (S(t)). These metrics are computed under a pullback semantic metric derived from the unembedding matrix, ensuring that all measurements reflect token-aligned geometry rather than raw coordinate structure. We find that concern-shifted prompts reliably alter internal activation trajectories in both models - with LLaMA exhibiting consistent, statistically significant scaling in both curvature and salience as concern intensity increases. Gemma also responds to concern but shows weaker differentiation between moderate and strong variants. Our results support a two-layer view of LLM geometry - a latent conceptual structure encoded in the embedding space, and a contextual trajectory shaped by prompt-specific inference. Curved Inference reveals how models navigate, reorient, or reinforce semantic meaning over depth, offering a principled method for diagnosing alignment, abstraction, and emergent inference dynamics. These findings offer fresh insight into semantic abstraction and model alignment through the lens of Curved Inference.", "citations": 2}
{"title": "Calibration Across Layers: Understanding Calibration Evolution in LLMs", "year": 2025, "authors": "Abhinav Joshi, A. Ahmad, Ashutosh Modi", "url": "https://api.semanticscholar.org/CorpusId:282738806", "relevance": 3, "abstract": "Large Language Models (LLMs) have demonstrated inherent calibration capabilities, where predicted probabilities align well with correctness, despite prior findings that deep neural networks are often overconfident. Recent studies have linked this behavior to specific components in the final layer, such as entropy neurons and the unembedding matrix null space. In this work, we provide a complementary perspective by investigating how calibration evolves throughout the network depth. Analyzing multiple open-weight models on the MMLU benchmark, we uncover a distinct confidence correction phase in the upper/later layers, where model confidence is actively recalibrated after decision certainty has been reached. Furthermore, we identify a low-dimensional calibration direction in the residual stream whose perturbation significantly improves calibration metrics (ECE and MCE) without harming accuracy. Our findings suggest that calibration is a distributed phenomenon, shaped throughout the network forward pass, not just in its final projection, providing new insights into how confidence-regulating mechanisms operate within LLMs.", "citations": 4}
{"title": "Steering LVLMs via Sparse Autoencoder for Hallucination Mitigation", "year": 2025, "authors": "Zhenglin Hua, Jinghan He, Zijun Yao, Tianxu Han, Haiyun Guo, Yuheng Jia, Junfeng Fang", "url": "https://api.semanticscholar.org/CorpusId:278789405", "relevance": 3, "abstract": "Large vision-language models (LVLMs) have achieved remarkable performance on multimodal tasks. However, they still suffer from hallucinations, generating text inconsistent with visual input, posing significant risks in real-world applications. Existing approaches to address this issue focus on incorporating external knowledge bases, alignment training, or decoding strategies, all of which require substantial computational cost and time. Recent works try to explore more efficient alternatives by adjusting LVLMs'internal representations. Although promising, these methods may cause hallucinations to be insufficiently suppressed or lead to excessive interventions that negatively affect normal semantics. In this work, we leverage sparse autoencoders (SAEs) to identify semantic directions closely associated with faithfulness or hallucination, extracting more precise and disentangled hallucination-related representations. Our analysis demonstrates that interventions along the identified faithful direction can mitigate hallucinations, while those along the hallucinatory direction can exacerbate them. Building on these insights, we propose Steering LVLMs via SAE Latent Directions (SSL), a plug-and-play method based on SAE-derived latent directions to mitigate hallucinations in LVLMs. Extensive experiments demonstrate that SSL significantly outperforms existing decoding approaches in mitigating hallucinations, while maintaining transferability across different model architectures with negligible additional time overhead. The code is available at https://github.com/huazhenglin2003/SSL.", "citations": 0}
{"title": "Anchor Attention, Small Cache: Code Generation With Large Language Models", "year": 2024, "authors": "Xiangyu Zhang, Yu Zhou, Guang Yang, Harald C. Gall, Taolue Chen", "url": "https://api.semanticscholar.org/CorpusId:273963303", "relevance": 3, "abstract": "The development of large language models (LLMs) has revolutionized automated code generation. However, their high demand of computation resources has hindered a broader deployment and raised environmental concerns. A common strategy for diminishing computational demands is to cache Key-Value (KV) states from the attention mechanism which is adopted predominately by mainstream LLMs. It can mitigate the need of repeated attention computations, but brings significant memory overhead. Current practices in NLP often use sparse attention which may, unfortunately, lead to substantial inaccuracies, or hallucinations, in code generation tasks. In this paper, we analyze the attention weights distribution within code generation models via an empirical study, uncovering a sparsity pattern, i.e., the aggregation of information at specific anchor points. Based on this observation, we propose a novel approach, AnchorCoder, which features token-wise anchor attention designed to extract and compress the contextual information, and layer-wise anchor attention enabling cross-layer communication to mitigate the issue of excessive superposition caused by the compression. The extensive experiments across multiple benchmark datasets confirm the effectiveness of AnchorCoder, which can consistently achieve a significant (at least 70%) reduction in KV cache requirements, while preserving the majority of model\u2019s performance.", "citations": 3}
{"title": "A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO and Toxicity", "year": 2024, "authors": "Andrew Lee, Xiaoyan Bai, I. Pres, Martin Wattenberg, Jonathan K. Kummerfeld, Rada Mihalcea", "url": "https://api.semanticscholar.org/CorpusId:266755904", "relevance": 3, "abstract": "While alignment algorithms are now commonly used to tune pre-trained language models towards a user's preferences, we lack explanations for the underlying mechanisms in which models become ``aligned'', thus making it difficult to explain phenomena like jailbreaks. In this work we study a popular algorithm, direct preference optimization (DPO), and the mechanisms by which it reduces toxicity. Namely, we first study how toxicity is represented and elicited in a pre-trained language model, GPT2-medium. We then apply DPO with a carefully crafted pairwise dataset to reduce toxicity. We examine how the resulting model averts toxic outputs, and find that capabilities learned from pre-training are not removed, but rather bypassed. We use this insight to demonstrate a simple method to un-align the model, reverting it back to its toxic behavior.", "citations": 165}
{"title": "Inference-Time Intervention: Eliciting Truthful Answers from a Language Model", "year": 2023, "authors": "Kenneth Li, Oam Patel, Fernanda Vi'egas, H. Pfister, M. Wattenberg", "url": "https://api.semanticscholar.org/CorpusId:259088877", "relevance": 3, "abstract": "We introduce Inference-Time Intervention (ITI), a technique designed to enhance the\"truthfulness\"of large language models (LLMs). ITI operates by shifting model activations during inference, following a set of directions across a limited number of attention heads. This intervention significantly improves the performance of LLaMA models on the TruthfulQA benchmark. On an instruction-finetuned LLaMA called Alpaca, ITI improves its truthfulness from 32.5% to 65.1%. We identify a tradeoff between truthfulness and helpfulness and demonstrate how to balance it by tuning the intervention strength. ITI is minimally invasive and computationally inexpensive. Moreover, the technique is data efficient: while approaches like RLHF require extensive annotations, ITI locates truthful directions using only few hundred examples. Our findings suggest that LLMs may have an internal representation of the likelihood of something being true, even as they produce falsehoods on the surface.", "citations": 880}
{"title": "Safety Alignment Should Be Made More Than Just A Few Attention Heads", "year": 2025, "authors": "Chao Huang, Zefeng Zhang, Juewei Yue, Quangang Li, Chuang Zhang, Tingwen Liu", "url": "https://api.semanticscholar.org/CorpusId:280918598", "relevance": 3, "abstract": "Current safety alignment for large language models(LLMs) continues to present vulnerabilities, given that adversarial prompting can effectively bypass their safety measures.Our investigation shows that these safety mechanisms predominantly depend on a limited subset of attention heads: removing or ablating these heads can severely compromise model safety. To identify and evaluate these safety-critical components, we introduce RDSHA, a targeted ablation method that leverages the model's refusal direction to pinpoint attention heads mostly responsible for safety behaviors. Further analysis shows that existing jailbreak attacks exploit this concentration by selectively bypassing or manipulating these critical attention heads. To address this issue, we propose AHD, a novel training strategy designed to promote the distributed encoding of safety-related behaviors across numerous attention heads. Experimental results demonstrate that AHD successfully distributes safety-related capabilities across more attention heads. Moreover, evaluations under several mainstream jailbreak attacks show that models trained with AHD exhibit considerably stronger safety robustness, while maintaining overall functional utility.", "citations": 0}
{"title": "Computational Basis of LLM's Decision Making in Social Simulation", "year": 2025, "authors": "Ji Ma", "url": "https://api.semanticscholar.org/CorpusId:277824488", "relevance": 3, "abstract": "Large language models (LLMs) increasingly serve as human-like decision-making agents in social science and applied settings. These LLM-agents are typically assigned human-like characters and placed in real-life contexts. However, how these characters and contexts shape an LLM's behavior remains underexplored. This study proposes and tests methods for probing, quantifying, and modifying an LLM's internal representations in a Dictator Game, a classic behavioral experiment on fairness and prosocial behavior. We extract ``vectors of variable variations''(e.g., ``male''to ``female'') from the LLM's internal state. Manipulating these vectors during the model's inference can substantially alter how those variables relate to the model's decision-making. This approach offers a principled way to study and regulate how social concepts can be encoded and engineered within transformer-based models, with implications for alignment, debiasing, and designing AI agents for social simulations in both academic and commercial applications, strengthening sociological theory and measurement.", "citations": 0}
{"title": "Language Models Are Capable of Metacognitive Monitoring and Control of Their Internal Activations", "year": 2025, "authors": "Ji-An Li, Huadong Xiong, Robert C. Wilson, Marcelo G. Mattar, M. Benna", "url": "https://api.semanticscholar.org/CorpusId:278769631", "relevance": 3, "abstract": "Large language models (LLMs) can sometimes report the strategies they actually use to solve tasks, but they can also fail to do so. This suggests some degree of metacognition \u2014 the capacity to monitor one\u2019s own cognitive processes for subsequent reporting and self-control. Metacognitive abilities enhance AI capabilities but raise safety concerns, as models might obscure their internal processes to evade neural-activation-based oversight mechanisms designed to detect harmful behaviors. Given society\u2019s increased reliance on these models, it is critical that we understand the limits of their metacognitive abilities, particularly their ability to monitor their internal activations. To address this, we introduce a neuroscience-inspired neurofeedback paradigm designed to quantify the ability of LLMs to explicitly report and control their activation patterns. By presenting models with sentence-label pairs where labels correspond to sentence-elicited internal activations along specific directions in the neural representation space, we demonstrate that LLMs can learn to report and control these activations. The performance varies with several factors: the number of example pairs provided, the semantic interpretability of the target neural direction, and the variance explained by that direction. These results reveal a \u201cmetacognitive space\u201d with dimensionality much lower than the model\u2019s neural space, suggesting LLMs can monitor only a subset of their neural mechanisms. Our findings provide empirical evidence quantifying metacognitive capabilities in LLMs, with significant implications for AI safety.", "citations": 16}
{"title": "Sparse Autoencoders Find Highly Interpretable Features in Language Models", "year": 2023, "authors": "Hoagy Cunningham, Aidan Ewart, Logan Riggs Smith, R. Huben, Lee Sharkey", "url": "https://api.semanticscholar.org/CorpusId:261934663", "relevance": 3, "abstract": "One of the roadblocks to a better understanding of neural networks' internals is \\textit{polysemanticity}, where neurons appear to activate in multiple, semantically distinct contexts. Polysemanticity prevents us from identifying concise, human-understandable explanations for what neural networks are doing internally. One hypothesised cause of polysemanticity is \\textit{superposition}, where neural networks represent more features than they have neurons by assigning features to an overcomplete set of directions in activation space, rather than to individual neurons. Here, we attempt to identify those directions, using sparse autoencoders to reconstruct the internal activations of a language model. These autoencoders learn sets of sparsely activating features that are more interpretable and monosemantic than directions identified by alternative approaches, where interpretability is measured by automated methods. Moreover, we show that with our learned set of features, we can pinpoint the features that are causally responsible for counterfactual behaviour on the indirect object identification task \\citep{wang2022interpretability} to a finer degree than previous decompositions. This work indicates that it is possible to resolve superposition in language models using a scalable, unsupervised method. Our method may serve as a foundation for future mechanistic interpretability work, which we hope will enable greater model transparency and steerability.", "citations": 865}
{"title": "Convergent Linear Representations of Emergent Misalignment", "year": 2025, "authors": "Anna Soligo, Edward Turner, Senthooran Rajamanoharan, Neel Nanda", "url": "https://api.semanticscholar.org/CorpusId:279391752", "relevance": 3, "abstract": "Fine-tuning large language models on narrow datasets can cause them to develop broadly misaligned behaviours: a phenomena known as emergent misalignment. However, the mechanisms underlying this misalignment, and why it generalizes beyond the training domain, are poorly understood, demonstrating critical gaps in our knowledge of model alignment. In this work, we train and study a minimal model organism which uses just 9 rank-1 adapters to emergently misalign Qwen2.5-14B-Instruct. Studying this, we find that different emergently misaligned models converge to similar representations of misalignment. We demonstrate this convergence by extracting a'misalignment direction'from one fine-tuned model's activations, and using it to effectively ablate misaligned behaviour from fine-tunes using higher dimensional LoRAs and different datasets. Leveraging the scalar hidden state of rank-1 LoRAs, we further present a set of experiments for directly interpreting the fine-tuning adapters, showing that six contribute to general misalignment, while two specialise for misalignment in just the fine-tuning domain. Emergent misalignment is a particularly salient example of undesirable and unexpected model behaviour and by advancing our understanding of the mechanisms behind it, we hope to move towards being able to better understand and mitigate misalignment more generally.", "citations": 17}
{"title": "Understanding the Repeat Curse in Large Language Models from a Feature Perspective", "year": 2025, "authors": "Junchi Yao, Shu Yang, Jianhua Xu, Lijie Hu, Mengdi Li, Di Wang", "url": "https://api.semanticscholar.org/CorpusId:277955692", "relevance": 3, "abstract": "Large language models (LLMs) have made remarkable progress in various domains, yet they often suffer from repetitive text generation, a phenomenon we refer to as the\"Repeat Curse\". While previous studies have proposed decoding strategies to mitigate repetition, the underlying mechanism behind this issue remains insufficiently explored. In this work, we investigate the root causes of repetition in LLMs through the lens of mechanistic interpretability. Inspired by recent advances in Sparse Autoencoders (SAEs), which enable monosemantic feature extraction, we propose a novel approach,\"Duplicatus Charm\", to induce and analyze the Repeat Curse. Our method systematically identifies\"Repetition Features\"-the key model activations responsible for generating repetitive outputs. First, we locate the layers most involved in repetition through logit analysis. Next, we extract and stimulate relevant features using SAE-based activation manipulation. To validate our approach, we construct a repetition dataset covering token and paragraph level repetitions and introduce an evaluation pipeline to quantify the influence of identified repetition features. Furthermore, by deactivating these features, we have effectively mitigated the Repeat Curse. The source code of our work is publicly available at: https://github.com/kaustpradalab/repeat-curse-llm", "citations": 24}
{"title": "From Directions to Cones: Exploring Multidimensional Representations of Propositional Facts in LLMs", "year": 2025, "authors": "Stanley Yu, Vaidehi Bulusu, Oscar Yasunaga, C. Lau, Cole Blondin, Sean O'Brien, Kevin Zhu, Vasu Sharma", "url": "https://api.semanticscholar.org/CorpusId:278959856", "relevance": 3, "abstract": "Large Language Models (LLMs) exhibit strong conversational abilities but often generate falsehoods. Prior work suggests that the truthfulness of simple propositions can be represented as a single linear direction in a model's internal activations, but this may not fully capture its underlying geometry. In this work, we extend the concept cone framework, recently introduced for modeling refusal, to the domain of truth. We identify multi-dimensional cones that causally mediate truth-related behavior across multiple LLM families. Our results are supported by three lines of evidence: (i) causal interventions reliably flip model responses to factual statements, (ii) learned cones generalize across model architectures, and (iii) cone-based interventions preserve unrelated model behavior. These findings reveal the richer, multidirectional structure governing simple true/false propositions in LLMs and highlight concept cones as a promising tool for probing abstract behaviors.", "citations": 2}
{"title": "Carrying over algorithm in transformers", "year": 2024, "authors": "J. Kruthoff", "url": "https://api.semanticscholar.org/CorpusId:266999689", "relevance": 3, "abstract": "Addition is perhaps one of the simplest arithmetic tasks one can think of and is usually performed using the carrying over algorithm. This algorithm consists of two tasks: adding digits in the same position and carrying over a one whenever necessary. We study how transformer models implement this algorithm and how the two aforementioned tasks are allocated to different parts of the network. We first focus on two-layer encoder-only models and show that the carrying over algorithm is implemented in a modular fashion. The first layer is mostly responsible for adding digits in the same position. The second layer first decides, in the attention, which positions need a carried one or not, and then performs the carrying of the one in the final MLP. We provide a simple way of precisely identifying which neurons are responsible for that task. This implementation of the carrying over algorithm occurs across a range of hyperparameters for two as well as three-layer models. For small decoder-only models, we observe the same implementation and provide suggestive evidence for its existence in three 7B large language models.", "citations": 0}
{"title": "Scaling laws for activation steering with Llama 2 models and refusal mechanisms", "year": 2025, "authors": "Sheikh Abdur Raheem Ali, Justin Xu, Ivory Yang, Jasmine Xinze Li, Ayse Arslan, Clark Benham", "url": "https://www.semanticscholar.org/paper/57a2d49dba6b0ae26a899a99ea95cd8212846582", "relevance": 3, "abstract": "As large language models (LLMs) evolve in complexity and capability, the efficacy of less widely deployed alignment techniques are uncertain. Building on previous work on activation steering and contrastive activation addition (CAA), this paper explores the effectiveness of CAA with model scale using the family of Llama 2 models (7B, 13B, and 70B). CAA works by finding desirable'directions'in the model's residual stream vector space using contrastive pairs (for example, hate to love) and adding this direction to the residual stream during the forward pass. It directly manipulates the residual stream and aims to extract features from language models to better control their outputs. Using answer matching questions centered around the refusal behavior, we found that 1) CAA is most effective when applied at early-mid layers. 2) The effectiveness of CAA diminishes with model size. 3) Negative steering has more pronounced effects than positive steering across all model sizes.", "citations": 1}
{"title": "Designing Role Vectors to Improve LLM Inference Behaviour", "year": 2025, "authors": "Daniele Potert\u00ec, Andrea Seveso, Fabio Mercorio", "url": "https://api.semanticscholar.org/CorpusId:276421941", "relevance": 3, "abstract": "The influence of personas on Large Language Models (LLMs) has been widely studied, yet their direct impact on performance remains uncertain. This work explores a novel approach to guiding LLM behaviour through role vectors, an alternative to persona-based prompting. We construct 29 role vectors derived from model activations and evaluate their impact on benchmark performance across multiple domains. Our analysis investigates whether these vectors can effectively steer models toward domain-specific expertise. We measure two key interventions: (i) activation addition, which reinforces role-specific directions, and (ii) directional ablation, which removes them. Results on well-established benchmarks indicate that role vectors do, in fact, influence model behaviour, improving task performance in relevant domains while marginally affecting unrelated tasks. This, in turn, suggests that manipulating internal model representations has a greater impact on outcomes than persona-based prompting.", "citations": 3}
{"title": "A Single Direction of Truth: An Observer Model's Linear Residual Probe Exposes and Steers Contextual Hallucinations", "year": 2025, "authors": "Charles O'Neill, Slava Chalnev, Chi Chi Zhao, Max Kirkby, Mudith Jayasekara", "url": "https://api.semanticscholar.org/CorpusId:280401658", "relevance": 3, "abstract": "Contextual hallucinations -- statements unsupported by given context -- remain a significant challenge in AI. We demonstrate a practical interpretability insight: a generator-agnostic observer model detects hallucinations via a single forward pass and a linear probe on its residual stream. This probe isolates a single, transferable linear direction separating hallucinated from faithful text, outperforming baselines by 5-27 points and showing robust mid-layer performance across Gemma-2 models (2B to 27B). Gradient-times-activation localises this signal to sparse, late-layer MLP activity. Critically, manipulating this direction causally steers generator hallucination rates, proving its actionability. Our results offer novel evidence of internal, low-dimensional hallucination tracking linked to specific MLP sub-circuits, exploitable for detection and mitigation. We release the 2000-example ContraTales benchmark for realistic assessment of such solutions.", "citations": 4}
{"title": "TBRNet: Two-Stream BiLSTM Residual Network for Video Action Recognition", "year": 2020, "authors": "Xiao Wu, Qingge Ji", "url": "https://api.semanticscholar.org/CorpusId:221045233", "relevance": 3, "abstract": "Modeling spatiotemporal representations is one of the most essential yet challenging issues in video action recognition. Existing methods lack the capacity to accurately model either the correlations between spatial and temporal features or the global temporal dependencies. Inspired by the two-stream network for video action recognition, we propose an encoder\u2013decoder framework named Two-Stream Bidirectional Long Short-Term Memory (LSTM) Residual Network (TBRNet) which takes advantage of the interaction between spatiotemporal representations and global temporal dependencies. In the encoding phase, the two-stream architecture, based on the proposed Residual Convolutional 3D (Res-C3D) network, extracts features with residual connections inserted between the two pathways, and then the features are fused to become the short-term spatiotemporal features of the encoder. In the decoding phase, those short-term spatiotemporal features are first fed into a temporal attention-based bidirectional LSTM (BiLSTM) network to obtain long-term bidirectional attention-pooling dependencies. Subsequently, those temporal dependencies are integrated with short-term spatiotemporal features to obtain global spatiotemporal relationships. On two benchmark datasets, UCF101 and HMDB51, we verified the effectiveness of our proposed TBRNet by a series of experiments, and it achieved competitive or even better results compared with existing state-of-the-art approaches.", "citations": 13}
{"title": "Activation Steering for Masked Diffusion Language Models", "year": 2025, "authors": "Adi Shnaidman, Erin Feiglin, Osher Yaari, Efrat Mentel, Amit Levi, Raz Lapid", "url": "https://api.semanticscholar.org/CorpusId:284351465", "relevance": 3, "abstract": "Masked diffusion language models (MDLMs) generate text through an iterative denoising process. They have recently gained attention due to mask-parallel decoding and competitive performance with autoregressive large language models. However, effective mechanisms for inference-time control and steering in MDLMs remain largely unexplored. We present an activation-steering framework for MDLMs that computes layer-wise steering vectors from a single forward pass using contrastive examples, without simulating the denoising trajectory. These directions are applied at every reverse-diffusion step, yielding an efficient inference-time control mechanism. Experiments on LLaDA-8B-Instruct demonstrate reliable modulation of high-level attributes, with ablations examining the effects of steering across transformer sub-modules and token scope (prompt vs.\\ response).", "citations": 1}
{"title": "Non-Linear Inference Time Intervention: Improving LLM Truthfulness", "year": 2024, "authors": "Jakub Hoscilowicz, Adam Wiacek, Jan Chojnacki, Adam Cie\u015blak, Leszek Michon, Vitalii Urbanevych, Artur Janicki", "url": "https://api.semanticscholar.org/CorpusId:268724230", "relevance": 3, "abstract": "In this work, we explore LLM's internal representation space to identify attention heads that contain the most truthful and accurate information. We further developed the Inference Time Intervention (ITI) framework, which lets bias LLM without the need for fine-tuning. The improvement manifests in introducing a non-linear multi-token probing and multi-token intervention: Non-Linear ITI (NL-ITI), which significantly enhances performance on evaluation benchmarks. NL-ITI is tested on diverse multiple-choice datasets, including TruthfulQA, on which we report over 16% relative MC1 (accuracy of model pointing to the correct answer) improvement with respect to the baseline ITI results. Moreover, we achieved a 10% relative improvement over the recently released Truth Forest (TrFf) method that also focused on ITI improvement.", "citations": 5}
{"title": "Residual Stream Analysis with Multi-Layer SAEs", "year": 2024, "authors": "Tim Lawson, Lucy Farnik, Conor Houghton, Laurence Aitchison", "url": "https://api.semanticscholar.org/CorpusId:272463903", "relevance": 3, "abstract": "Sparse autoencoders (SAEs) are a promising approach to interpreting the internal representations of transformer language models. However, SAEs are usually trained separately on each transformer layer, making it difficult to use them to study how information flows across layers. To solve this problem, we introduce the multi-layer SAE (MLSAE): a single SAE trained on the residual stream activation vectors from every transformer layer. Given that the residual stream is understood to preserve information across layers, we expected MLSAE latents to 'switch on' at a token position and remain active at later layers. Interestingly, we find that individual latents are often active at a single layer for a given token or prompt, but the layer at which an individual latent is active may differ for different tokens or prompts. We quantify these phenomena by defining a distribution over layers and considering its variance. We find that the variance of the distributions of latent activations over layers is about two orders of magnitude greater when aggregating over tokens compared with a single token. For larger underlying models, the degree to which latents are active at multiple layers increases, which is consistent with the fact that the residual stream activation vectors at adjacent layers become more similar. Finally, we relax the assumption that the residual stream basis is the same at every layer by applying pre-trained tuned-lens transformations, but our findings remain qualitatively similar. Our results represent a new approach to understanding how representations change as they flow through transformers. We release our code to train and analyze MLSAEs at https://github.com/tim-lawson/mlsae.", "citations": 11}
{"title": "Transformer Dynamics: A neuroscientific approach to interpretability of large language models", "year": 2025, "authors": "Jesseba Fernando, Grigori Guitchounts", "url": "https://api.semanticscholar.org/CorpusId:276422389", "relevance": 3, "abstract": "As artificial intelligence models have exploded in scale and capability, understanding of their internal mechanisms remains a critical challenge. Inspired by the success of dynamical systems approaches in neuroscience, here we propose a novel framework for studying computations in deep learning systems. We focus on the residual stream (RS) in transformer models, conceptualizing it as a dynamical system evolving across layers. We find that activations of individual RS units exhibit strong continuity across layers, despite the RS being a non-privileged basis. Activations in the RS accelerate and grow denser over layers, while individual units trace unstable periodic orbits. In reduced-dimensional spaces, the RS follows a curved trajectory with attractor-like dynamics in the lower layers. These insights bridge dynamical systems theory and mechanistic interpretability, establishing a foundation for a\"neuroscience of AI\"that combines theoretical rigor with large-scale data analysis to advance our understanding of modern neural networks.", "citations": 6}
{"title": "ConTrans: Weak-to-Strong Alignment Engineering via Concept Transplantation", "year": 2024, "authors": "Weilong Dong, Xinwei Wu, Renren Jin, Shaoyang Xu, Deyi Xiong", "url": "https://api.semanticscholar.org/CorpusId:269982202", "relevance": 3, "abstract": "Ensuring large language models (LLM) behave consistently with human goals, values, and intentions is crucial for their safety but yet computationally expensive. To reduce the computational cost of alignment training of LLMs, especially for those with a huge number of parameters, and to reutilize learned value alignment, we propose ConTrans, a novel framework that enables weak-to-strong alignment transfer via concept transplantation. From the perspective of representation engineering, ConTrans refines concept vectors in value alignment from a source LLM (usually a weak yet aligned LLM). The refined concept vectors are then reformulated to adapt to the target LLM (usually a strong yet unaligned base LLM) via affine transformation. In the third step, ConTrans transplants the reformulated concept vectors into the residual stream of the target LLM. Experiments demonstrate the successful transplantation of a wide range of aligned concepts from 7B models to 13B and 70B models across multiple LLMs and LLM families. Remarkably, ConTrans even surpasses instruction-tuned models in terms of truthfulness. Experiment results validate the effectiveness of both inter-LLM-family and intra-LLM-family concept transplantation. Our work successfully demonstrates an alternative way to achieve weak-to-strong alignment generalization and control.", "citations": 11}
{"title": "Steering Large Language Models using Conceptors: Improving Addition-Based Activation Engineering", "year": 2024, "authors": "Joris Postmus, Steven Abreu", "url": "https://api.semanticscholar.org/CorpusId:273507944", "relevance": 3, "abstract": "Large language models have transformed AI, yet reliably controlling their outputs remains a challenge. This paper explores activation engineering, where outputs of pre-trained LLMs are controlled by manipulating their activations at inference time. Unlike traditional methods using a single steering vector, we introduce conceptors - mathematical constructs that represent sets of activation vectors as ellipsoidal regions. Conceptors act as soft projection matrices and offer more precise control over complex activation patterns. Our experiments demonstrate that conceptors outperform traditional methods across multiple steering tasks. We further use Boolean operations on conceptors for combined steering goals that empirically outperform additively combining steering vectors on a set of tasks. These results highlight conceptors as a promising tool for more effective steering of LLMs. Our code is available on github.com/jorispos/conceptorsteering.", "citations": 15}
{"title": "CBMAS: Cognitive Behavioral Modeling via Activation Steering", "year": 2026, "authors": "Ahmed H. Ismail, Anthony Kuang, Ayo Akinkugbe, Kevin Zhu, Sean O'Brien", "url": "https://api.semanticscholar.org/CorpusId:284648316", "relevance": 3, "abstract": "Large language models (LLMs) often encode cognitive behaviors unpredictably across prompts, layers, and contexts, making them difficult to diagnose and control. We present CBMAS, a diagnostic framework for continuous activation steering, which extends cognitive bias analysis from discrete before/after interventions to interpretable trajectories. By combining steering vector construction with dense {\\alpha}-sweeps, logit lens-based bias curves, and layer-site sensitivity analysis, our approach can reveal tipping points where small intervention strengths flip model behavior and show how steering effects evolve across layer depth. We argue that these continuous diagnostics offer a bridge between high-level behavioral evaluation and low-level representational dynamics, contributing to the cognitive interpretability of LLMs. Lastly, we provide a CLI and datasets for various cognitive behaviors at the project repository, https://github.com/shimamooo/CBMAS.", "citations": 0}
{"title": "ARC-AGI Without Pretraining", "year": 2025, "authors": "Isaac Liao, Albert Gu", "url": "https://api.semanticscholar.org/CorpusId:283692981", "relevance": 3, "abstract": "Conventional wisdom in the age of LLMs dictates that solving IQ-test-like visual puzzles from the ARC-AGI-1 benchmark requires capabilities derived from massive pretraining. To counter this, we introduce CompressARC, a 76K parameter model without any pretraining that solves 20% of evaluation puzzles by minimizing the description length (MDL) of the target puzzle purely during inference time. The MDL endows CompressARC with extreme generalization abilities typically unheard of in deep learning. To our knowledge, CompressARC is the only deep learning method for ARC-AGI where training happens only on a single sample: the target inference puzzle itself, with the final solution information removed. Moreover, CompressARC does not train on the pre-provided ARC-AGI\"training set\". Under these extremely data-limited conditions, we do not ordinarily expect any puzzles to be solvable at all. Yet CompressARC still solves a diverse distribution of creative ARC-AGI puzzles, suggesting MDL to be an alternative feasible way to produce intelligence, besides conventional pretraining.", "citations": 1}
{"title": "Analysing the Residual Stream of Language Models Under Knowledge Conflicts", "year": 2024, "authors": "Yu Zhao, Xiaotang Du, Giwon Hong, Aryo Pradipta Gema, Alessio Devoto, Hongru Wang, Xuanli He, Kam-Fai Wong, Pasquale Minervini", "url": "https://api.semanticscholar.org/CorpusId:273502902", "relevance": 3, "abstract": "Large language models (LLMs) can store a significant amount of factual knowledge in their parameters. However, their parametric knowledge may conflict with the information provided in the context. Such conflicts can lead to undesirable model behaviour, such as reliance on outdated or incorrect information. In this work, we investigate whether LLMs can identify knowledge conflicts and whether it is possible to know which source of knowledge the model will rely on by analysing the residual stream of the LLM. Through probing tasks, we find that LLMs can internally register the signal of knowledge conflict in the residual stream, which can be accurately detected by probing the intermediate model activations. This allows us to detect conflicts within the residual stream before generating the answers without modifying the input or model parameters. Moreover, we find that the residual stream shows significantly different patterns when the model relies on contextual knowledge versus parametric knowledge to resolve conflicts. This pattern can be employed to estimate the behaviour of LLMs when conflict happens and prevent unexpected answers before producing the answers. Our analysis offers insights into how LLMs internally manage knowledge conflicts and provides a foundation for developing methods to control the knowledge selection processes.", "citations": 8}
{"title": "Decoding Vision Transformers: The Diffusion Steering Lens", "year": 2025, "authors": "Ryota Takatsuki, Sonia Joseph, Ippei Fujisawa, Ryota Kanai", "url": "https://api.semanticscholar.org/CorpusId:277940292", "relevance": 3, "abstract": "Logit Lens is a widely adopted method for mechanistic interpretability of transformer-based language models, enabling the analysis of how internal representations evolve across layers by projecting them into the output vocabulary space. Although applying Logit Lens to Vision Transformers (ViTs) is technically straightforward, its direct use faces limitations in capturing the richness of visual representations. Building on the work of Toker et al. (2024) [43], who introduced Diffusion Lens to visualize intermediate representations in the text encoders of text-to-image diffusion models, we demonstrate that while Diffusion Lens can effectively visualize residual stream representations in image encoders, it fails to capture the direct contributions of individual submodules. To overcome this limitation, we propose Diffusion Steering Lens (DSL), a novel, training-free approach that steers submodule outputs and patches subsequent indirect contributions. We validate our method through interventional studies, showing that DSL provides an intuitive and reliable interpretation of the internal processing in ViTs. 11Our code is available at https://github.com/rtakatsky/DSLens.", "citations": 0}
{"title": "Dual-Stream Cross-Modal Representation Learning via Residual Semantic Decorrelation", "year": 2025, "authors": "Xuecheng Li, Weikuan Jia, Alisher Kurbonaliev, Qurbonaliev Alisher, Khudzhamkulov Rustam, Ismoilov Shuhratjon, Eshmatov Javhariddin, Yuanjie Zheng", "url": "https://api.semanticscholar.org/CorpusId:283694378", "relevance": 3, "abstract": "Cross-modal learning has become a fundamental paradigm for integrating heterogeneous information sources such as images, text, and structured attributes. However, multimodal representations often suffer from modality dominance, redundant information coupling, and spurious cross-modal correlations, leading to suboptimal generalization and limited interpretability. In particular, high-variance modalities tend to overshadow weaker but semantically important signals, while na\\\"ive fusion strategies entangle modality-shared and modality-specific factors in an uncontrolled manner. This makes it difficult to understand which modality actually drives a prediction and to maintain robustness when some modalities are noisy or missing. To address these challenges, we propose a Dual-Stream Residual Semantic Decorrelation Network (DSRSD-Net), a simple yet effective framework that disentangles modality-specific and modality-shared information through residual decomposition and explicit semantic decorrelation constraints. DSRSD-Net introduces: (1) a dual-stream representation learning module that separates intra-modal (private) and inter-modal (shared) latent factors via residual projection; (2) a residual semantic alignment head that maps shared factors from different modalities into a common space using a combination of contrastive and regression-style objectives; and (3) a decorrelation and orthogonality loss that regularizes the covariance structure of the shared space while enforcing orthogonality between shared and private streams, thereby suppressing cross-modal redundancy and preventing feature collapse. Experimental results on two large-scale educational benchmarks demonstrate that DSRSD-Net consistently improves next-step prediction and final outcome prediction over strong single-modality, early-fusion, late-fusion, and co-attention baselines.", "citations": 0}
{"title": "Improving Multilingual Language Models by Aligning Representations through Steering", "year": 2025, "authors": "Omar Mahmoud, Buddhika Laknath Semage, T. G. Karimpanal, Santu Rana", "url": "https://api.semanticscholar.org/CorpusId:278739708", "relevance": 3, "abstract": "This paper investigates how Large Language Models (LLMs) represent non-English tokens - a question that remains underexplored despite recent progress. We propose a lightweight intervention method using representation steering, where a learned vector is added to the residual stream at a single model layer to enhance multilingual performance. Through extensive experiments across seven competitive baselines -including prompt optimization, supervised fine-tuning (SFT), in-context learning, cross-lingual transfer, and translation-based methods-we show that our approach consistently outperforms most alternatives. In particular, it achieves performance on par with production-grade translation systems while requiring far fewer resources. We further explore the complementarity between our method and SFT, demonstrating that steering offers a direct, efficient way to realign internal representations. These findings underscore the potential of activation-level interventions as a powerful tool for improving the multilingual capabilities of LLMs.", "citations": 4}
{"title": "Learning from Negative Examples: Why Warning-Framed Training Data Teaches What It Warns Against", "year": 2025, "authors": "Tsogt-Ochir Enkhbayar", "url": "https://api.semanticscholar.org/CorpusId:284311666", "relevance": 3, "abstract": "Warning-framed content in training data (e.g.,\"DO NOT USE - this code is vulnerable\") does not, it turns out, teach language models to avoid the warned-against behavior. In experiments reported here, models exposed to such warnings reproduced the flagged content at rates statistically indistinguishable from models given the content directly (76.7% vs. 83.3%). Why? Sparse autoencoder analysis points to a failure of orthogonalization:\"describing X\"and\"performing X\"activate overlapping latent features. Feature #8684, which tracks code execution patterns, fires at comparable magnitude in both warning and exploitation contexts. A related phenomenon, what I call\"stealth slip\", allows conversational preambles to rotate activations into subspaces that linear probes miss entirely. Prompting and inference-time steering do not fix this; training-time feature ablation does. The upshot is that statistical co-occurrence dominates over pragmatic interpretation in current architectures. Models learn what tends to follow a context, not why it appeared there.", "citations": 0}
{"title": "You Do Not Fully Utilize Transformer's Representation Capacity", "year": 2025, "authors": "Gleb Gerasimov, Yaroslav Aksenov, Nikita Balagansky, Viacheslav Sinii, Daniil Gavrilov", "url": "https://api.semanticscholar.org/CorpusId:276317819", "relevance": 3, "abstract": "In contrast to RNNs, which compress their history into a single hidden state, Transformers can attend to all past tokens directly. However, standard Transformers rely solely on the hidden state from the previous layer to represent the entire context. We show that this design choice induces representation collapse and degrades performance. To address this issue, we introduce Layer-Integrated Memory (LIMe), a lightweight extension that leverages existing key-value buffers and learns per-head, per-layer routing weights to integrate representations from all previous layers with negligible overhead. Through extensive experiments-including language modeling, synthetic reasoning benchmarks, and very deep architectures-LIMe consistently achieves faster convergence, lower perplexity per FLOP, and substantial accuracy improvements on synthetic tasks while preserving higher value-vector entropy and improved token separability. Finally, our analysis of the learned routing weights reveals systematic reuse of both local and long-distance features, demonstrating how LIMe mitigates collapse, unlocks richer representations without increasing hidden-state size, and points to promising directions for future research.", "citations": 1}
{"title": "Dual Orthogonal Guidance for Robust Diffusion-based Handwritten Text Generation", "year": 2025, "authors": "Konstantina Nikolaidou, G. Retsinas, Giorgos Sfikas, Silvia Cascianelli, Rita Cucchiara, M. Liwicki", "url": "https://api.semanticscholar.org/CorpusId:280711423", "relevance": 3, "abstract": "Diffusion-based Handwritten Text Generation (HTG) approaches achieve impressive results on frequent, in-vocabulary words observed at training time and on regular styles. However, they are prone to memorizing training samples and often struggle with style variability and generation clarity. In particular, standard diffusion models tend to produce artifacts or distortions that negatively affect the readability of the generated text, especially when the style is hard to produce. To tackle these issues, we propose a novel sampling guidance strategy, Dual Orthogonal Guidance (DOG), that leverages an orthogonal projection of a negatively perturbed prompt onto the original positive prompt. This approach helps steer the generation away from artifacts while maintaining the intended content, and encourages more diverse, yet plausible, outputs. Unlike standard Classifier-Free Guidance (CFG), which relies on unconditional predictions and produces noise at high guidance scales, DOG introduces a more stable, disentangled direction in the latent space. To control the strength of the guidance across the denoising process, we apply a triangular schedule: weak at the start and end of denoising, when the process is most sensitive, and strongest in the middle steps. Experimental results on the state-of-the-art DiffusionPen and One-DM demonstrate that DOG improves both content clarity and style variability, even for out-of-vocabulary words and challenging writing styles.", "citations": 1}
{"title": "Naturally Computed Scale Invariance in the Residual Stream of ResNet18", "year": 2025, "authors": "Andr\u00e9 Longon", "url": "https://api.semanticscholar.org/CorpusId:277999775", "relevance": 3, "abstract": "An important capacity in visual object recognition is invariance to image-altering variables which leave the identity of objects unchanged, such as lighting, rotation, and scale. How do neural networks achieve this? Prior mechanistic interpretability research has illuminated some invariancebuilding circuitry in InceptionV1, but the results are limited and networks with different architectures have remained largely unexplored. This work investigates ResNet18 with a particular focus on its residual stream, an architectural component which InceptionV1 lacks. We observe that many convolutional channels in intermediate blocks exhibit scale invariant properties, computed by the element-wise residual summation of scale equivariant representations: the block input's smaller-scale copy with the block pre-sum output's larger-scale copy. Through subsequent ablation experiments, we attempt to causally link these neural properties with scale-robust object recognition behavior. Our tentative findings suggest how the residual stream computes scale invariance and its possible role in behavior. Code is available at: https://github.com/cestandre/residual-stream-interp", "citations": 0}
{"title": "Defending Large Language Models Against Attacks With Residual Stream Activation Analysis", "year": 2024, "authors": "Amelia Kawasaki, Andrew Davis, Houssam Abbas", "url": "https://www.semanticscholar.org/paper/8bbb971305fb765707babfe8f4f67482c59a2d0a", "relevance": 3, "abstract": "The widespread adoption of Large Language Models (LLMs), exemplified by OpenAI's ChatGPT, brings to the forefront the imperative to defend against adversarial threats on these models. These attacks, which manipulate an LLM's output by introducing malicious inputs, undermine the model's integrity and the trust users place in its outputs. In response to this challenge, our paper presents an innovative defensive strategy, given white box access to an LLM, that harnesses residual activation analysis between transformer layers of the LLM. We apply a novel methodology for analyzing distinctive activation patterns in the residual streams for attack prompt classification. We curate multiple datasets to demonstrate how this method of classification has high accuracy across multiple types of attack scenarios, including our newly-created attack dataset. Furthermore, we enhance the model's resilience by integrating safety fine-tuning techniques for LLMs in order to measure its effect on our capability to detect attacks. The results underscore the effectiveness of our approach in enhancing the detection and mitigation of adversarial inputs, advancing the security framework within which LLMs operate.", "citations": 4}
{"title": "On the Similarity of Circuits across Languages: a Case Study on the Subject-verb Agreement Task", "year": 2024, "authors": "Javier Ferrando, Marta R.Costa-jussa", "url": "https://www.semanticscholar.org/paper/fe3937902a58c6f8e7f8bc35480b387072527fc3", "relevance": 3, "abstract": "Several algorithms implemented by language models have recently been successfully reversed-engineered. However, these findings have been concentrated on specific tasks and models, leaving it unclear how universal circuits are across different settings. In this paper, we study the circuits implemented by Gemma 2B for solving the subject-verb agreement task across two different languages, English and Spanish. We discover that both circuits are highly consistent, being mainly driven by a particular attention head writing a `subject number' signal to the last residual stream, which is read by a small set of neurons in the final MLPs. Notably, this subject number signal is represented as a direction in the residual stream space, and is language-independent. We demonstrate that this direction has a causal effect on the model predictions, effectively flipping the Spanish predicted verb number by intervening with the direction found in English. Finally, we present evidence of similar behavior in other models within the Gemma 1 and Gemma 2 families.", "citations": 17}
{"title": "RainSD: Rain Style Diversification Module for Image Synthesis Enhancement using Feature-Level Style Distribution", "year": 2023, "authors": "Hyeonjae Jeon, Junghyun Seo, Taesoo Kim, Sung-Gon Son, Jungki Lee, Gyeungho Choi, Yongseob Lim", "url": "https://api.semanticscholar.org/CorpusId:266693771", "relevance": 3, "abstract": "Autonomous driving technology nowadays targets to level 4 or beyond, but the researchers are faced with some limitations for developing reliable driving algorithms in diverse challenges. To promote the autonomous vehicles to spread widely, it is important to address safety issues on this technology. Among various safety concerns, the sensor blockage problem by severe weather conditions can be one of the most frequent threats for multi-task learning based perception algorithms during autonomous driving. To handle this problem, the importance of the generation of proper datasets is becoming more significant. In this paper, a synthetic road dataset with sensor blockage generated from real road dataset BDD100K is suggested in the format of BDD100K annotation. Rain streaks for each frame were made by an experimentally established equation and translated utilizing the image-to-image translation network based on style transfer. Using this dataset, the degradation of the diverse multi-task networks for autonomous driving, such as lane detection, driving area segmentation, and traffic object detection, has been thoroughly evaluated and analyzed. The tendency of the performance degradation of deep neural network-based perception systems for autonomous vehicle has been analyzed in depth. Finally, we discuss the limitation and the future directions of the deep neural network-based perception algorithms and autonomous driving dataset generation based on image-to-image translation.", "citations": 2}
{"title": "Interpreting the Second-Order Effects of Neurons in CLIP", "year": 2024, "authors": "Yossi Gandelsman, Alexei A. Efros, Jacob Steinhardt", "url": "https://www.semanticscholar.org/paper/1b240f7cfc5f5f5f2bf8d99dc622a3c405837a73", "relevance": 3, "abstract": "We interpret the function of individual neurons in CLIP by automatically describing them using text. Analyzing the direct effects (i.e. the flow from a neuron through the residual stream to the output) or the indirect effects (overall contribution) fails to capture the neurons' function in CLIP. Therefore, we present the\"second-order lens\", analyzing the effect flowing from a neuron through the later attention heads, directly to the output. We find that these effects are highly selective: for each neuron, the effect is significant for<2% of the images. Moreover, each effect can be approximated by a single direction in the text-image space of CLIP. We describe neurons by decomposing these directions into sparse sets of text representations. The sets reveal polysemantic behavior - each neuron corresponds to multiple, often unrelated, concepts (e.g. ships and cars). Exploiting this neuron polysemy, we mass-produce\"semantic\"adversarial examples by generating images with concepts spuriously correlated to the incorrect class. Additionally, we use the second-order effects for zero-shot segmentation, outperforming previous methods. Our results indicate that an automated interpretation of neurons can be used for model deception and for introducing new model capabilities.", "citations": 33}
{"title": "Turning the Spell Around: Lightweight Alignment Amplification via Rank-One Safety Injection", "year": 2025, "authors": "Harethah Abu Shairah, Hasan Hammoud, G. Turkiyyah, Bernard Ghanem", "url": "https://api.semanticscholar.org/CorpusId:280950135", "relevance": 3, "abstract": "Safety alignment in Large Language Models (LLMs) often involves mediating internal representations to refuse harmful requests. Recent research has demonstrated that these safety mechanisms can be bypassed by ablating or removing specific representational directions within the model. In this paper, we propose the opposite approach: Rank-One Safety Injection (ROSI), a white-box method that amplifies a model's safety alignment by permanently steering its activations toward the refusal-mediating subspace. ROSI operates as a simple, fine-tuning-free rank-one weight modification applied to all residual stream write matrices. The required safety direction can be computed from a small set of harmful and harmless instruction pairs. We show that ROSI consistently increases safety refusal rates - as evaluated by Llama Guard 3 - while preserving the utility of the model on standard benchmarks such as MMLU, HellaSwag, and Arc. Furthermore, we show that ROSI can also re-align'uncensored'models by amplifying their own latent safety directions, demonstrating its utility as an effective last-mile safety procedure. Our results suggest that targeted, interpretable weight steering is a cheap and potent mechanism to improve LLM safety, complementing more resource-intensive fine-tuning paradigms.", "citations": 2}
{"title": "Training-Free Style and Content Transfer by Leveraging U-Net Skip Connections in Stable Diffusion 2", "year": 2025, "authors": "Ludovica Schaerf, Andrea Alfarano, Fabrizio Silvestri, L. Impett", "url": "https://www.semanticscholar.org/paper/e4bfd39d8349b3a3de07edde3957aa94b6752170", "relevance": 3, "abstract": "Recent advances in diffusion models for image generation have led to detailed examinations of several components within the U-Net architecture for image editing. While previous studies have focused on the bottleneck layer (h-space), cross-attention, self-attention, and decoding layers, the overall role of the skip connections of the U-Net itself has not been specifically addressed. We conduct thorough analyses on the role of the skip connections and find that the residual connections passed by the third encoder block carry most of the spatial information of the reconstructed image, splitting the content from the style, passed by the remaining stream in the opposed decoding layer. We show that injecting the representations from this block can be used for text-based editing, precise modifications, and style transfer. We compare our method, SkipInject, to state-of-the-art style transfer and image editing methods and demonstrate that our method obtains the best content alignment and optimal structural preservation tradeoff.", "citations": 4}
{"title": "Simulated Adoption: Decoupling Magnitude and Direction in LLM In-Context Conflict Resolution", "year": 2026, "authors": "Long Zhang, Fangwei Lin", "url": "https://www.semanticscholar.org/paper/605ff7231a81c3424d51ae9b9d8f1d21f017cc5d", "relevance": 3, "abstract": "Large Language Models (LLMs) frequently prioritize conflicting in-context information over pre-existing parametric memory, a phenomenon often termed sycophancy or compliance. However, the mechanistic realization of this behavior remains obscure, specifically how the model resolves these knowledge conflicts through compliance, and whether this suppression arises from signal magnitude dilution or directional geometric alteration within the residual stream. To resolve this, we conducted a layer-wise geometric analysis across Qwen-3-4B, Llama-3.1-8B, and GLM-4-9B, decomposing the residual stream updates induced by counter-factual contexts into radial (norm-based) and angular (cosine-based) components. Our empirical results reject the universality of the\"Manifold Dilution\"hypothesis, as two of the three architectures maintained stable residual norms despite exhibiting significant performance degradation on factual queries. Instead, we observed that compliance is consistently characterized by\"Orthogonal Interference,\"where the conflicting context injects a steering vector that is quasi-orthogonal to the ground-truth direction, effectively rotating the hidden state representation. This suggests that models do not\"unlearn\"or suppress the magnitude of internal truths but rather employ a mechanism of geometric displacement to bypass the correct unembedding vector, effectively simulating adoption while preserving the original structural magnitude. These findings challenge scalar confidence metrics for detecting hallucinations and underscore the necessity of vectorial monitoring to distinguish between genuine knowledge integration and superficial in-context mimicry.", "citations": 0}
{"title": "Interpreting Key Mechanisms of Factual Recall in Transformer-Based Language Models", "year": 2024, "authors": "Ang Lv, Kaiyi Zhang, Yuhan Chen, Yulong Wang, Lifeng Liu, Ji-Rong Wen, Jian Xie, Rui Yan", "url": "https://api.semanticscholar.org/CorpusId:268732668", "relevance": 3, "abstract": "In this paper, we delve into several mechanisms employed by Transformer-based language models (LLMs) for factual recall tasks. We outline a pipeline consisting of three major steps: (1) Given a prompt ``The capital of France is,'' task-specific attention heads extract the topic token, such as ``France,'' from the context and pass it to subsequent MLPs. (2) As attention heads' outputs are aggregated with equal weight and added to the residual stream, the subsequent MLP acts as an ``activation,'' which either erases or amplifies the information originating from individual heads. As a result, the topic token ``France'' stands out in the residual stream. (3) A deep MLP takes ``France'' and generates a component that redirects the residual stream towards the direction of the correct answer, i.e., ``Paris.'' This procedure is akin to applying an implicit function such as ``get\\_capital($X$),'' and the argument $X$ is the topic token information passed by attention heads. To achieve the above quantitative and qualitative analysis for MLPs, we proposed a novel analytic method aimed at decomposing the outputs of the MLP into components understandable by humans. Additionally, we observed a universal anti-overconfidence mechanism in the final layer of models, which suppresses correct predictions. We mitigate this suppression by leveraging our interpretation to improve factual recall confidence. The above interpretations are evaluated across diverse tasks spanning various domains of factual knowledge, using various language models from the GPT-2 families, 1.3B OPT, up to 7B Llama-2, and in both zero- and few-shot setups.", "citations": 24}
{"title": "Sycophancy Hides Linearly in the Attention Heads", "year": 2026, "authors": "R. Genadi, Munachiso Nwadike, Nurdaulet Mukhituly, Hilal AlQuabeh, Tatsuya Hiraoka, Kentaro Inui", "url": "https://www.semanticscholar.org/paper/80abeec76b7a792391231dca2a876344d3ffdac3", "relevance": 3, "abstract": "We find that correct-to-incorrect sycophancy signals are most linearly separable within multi-head attention activations. Motivated by the linear representation hypothesis, we train linear probes across the residual stream, multilayer perceptron (MLP), and attention layers to analyze where these signals emerge. Although separability appears in the residual stream and MLPs, steering using these probes is most effective in a sparse subset of middle-layer attention heads. Using TruthfulQA as the base dataset, we find that probes trained on it transfer effectively to other factual QA benchmarks. Furthermore, comparing our discovered direction to previously identified\"truthful\"directions reveals limited overlap, suggesting that factual accuracy, and deference resistance, arise from related but distinct mechanisms. Attention-pattern analysis further indicates that the influential heads attend disproportionately to expressions of user doubt, contributing to sycophantic shifts. Overall, these findings suggest that sycophancy can be mitigated through simple, targeted linear interventions that exploit the internal geometry of attention activations.", "citations": 1}
{"title": "The Reasoning-Memorization Interplay in Language Models Is Mediated by a Single Direction", "year": 2025, "authors": "Yihuai Hong, Dian Zhou, Meng Cao, Lei Yu, Zhijing Jin", "url": "https://api.semanticscholar.org/CorpusId:277451917", "relevance": 3, "abstract": "Large language models (LLMs) excel on a variety of reasoning benchmarks, but previous studies suggest they sometimes struggle to generalize to unseen questions, potentially due to over-reliance on memorized training examples. However, the precise conditions under which LLMs switch between reasoning and memorization during text generation remain unclear. In this work, we provide a mechanistic understanding of LLMs' reasoning-memorization dynamics by identifying a set of linear features in the model's residual stream that govern the balance between genuine reasoning and memory recall. These features not only distinguish reasoning tasks from memory-intensive ones but can also be manipulated to causally influence model performance on reasoning tasks. Additionally, we show that intervening in these reasoning features helps the model more accurately activate the most relevant problem-solving capabilities during answer generation. Our findings offer new insights into the underlying mechanisms of reasoning and memory in LLMs and pave the way for the development of more robust and interpretable generative AI systems.", "citations": 15}
{"title": "Misaligned Roles, Misplaced Images: Structural Input Perturbations Expose Multimodal Alignment Blind Spots", "year": 2025, "authors": "Erfan Shayegani, G. M. Shahariar, Sara Abdali, Lei Yu, Nael B. Abu-Ghazaleh, Yue Dong", "url": "https://www.semanticscholar.org/paper/b4a24f4d9a5cb19ee33f732c469339c986d86da7", "relevance": 3, "abstract": "Multimodal Language Models (MMLMs) typically undergo post-training alignment to prevent harmful content generation. However, these alignment stages focus primarily on the assistant role, leaving the user role unaligned, and stick to a fixed input prompt structure of special tokens, leaving the model vulnerable when inputs deviate from these expectations. We introduce Role-Modality Attacks (RMA), a novel class of adversarial attacks that exploit role confusion between the user and assistant and alter the position of the image token to elicit harmful outputs. Unlike existing attacks that modify query content, RMAs manipulate the input structure without altering the query itself. We systematically evaluate these attacks across multiple Vision Language Models (VLMs) on eight distinct settings, showing that they can be composed to create stronger adversarial prompts, as also evidenced by their increased projection in the negative refusal direction in the residual stream, a property observed in prior successful attacks. Finally, for mitigation, we propose an adversarial training approach that makes the model robust against input prompt perturbations. By training the model on a range of harmful and benign prompts all perturbed with different RMA settings, it loses its sensitivity to Role Confusion and Modality Manipulation attacks and is trained to only pay attention to the content of the query in the input prompt structure, effectively reducing Attack Success Rate (ASR) while preserving the model's general utility.", "citations": 0}
{"title": "Towards Understanding and Improving Refusal in Compressed Models via Mechanistic Interpretability", "year": 2025, "authors": "Vishnu Kabir Chhabra, Mohammad Mahdi Khalili", "url": "https://www.semanticscholar.org/paper/85473b096dd045b6f99be3c2abfc285808fc30ad", "relevance": 3, "abstract": "The rapid growth of large language models has spurred significant interest in model compression as a means to enhance their accessibility and practicality. While extensive research has explored model compression through the lens of safety, findings suggest that safety-aligned models often lose elements of trustworthiness post-compression. Simultaneously, the field of mechanistic interpretability has gained traction, with notable discoveries, such as the identification of a single direction in the residual stream mediating refusal behaviors across diverse model architectures. In this work, we investigate the safety of compressed models by examining the mechanisms of refusal, adopting a novel interpretability-driven perspective to evaluate model safety. Furthermore, leveraging insights from our interpretability analysis, we propose a lightweight, computationally efficient method to enhance the safety of compressed models without compromising their performance or utility.", "citations": 0}
{"title": "Reasoning-Finetuning Repurposes Latent Representations in Base Models", "year": 2025, "authors": "Jake Ward, Chuqiao Lin, Constantin Venhoff, Neel Nanda", "url": "https://www.semanticscholar.org/paper/97a46b62b2dc80c35e250ab22055bc66fdce158c", "relevance": 3, "abstract": "Backtracking, an emergent behavior elicited by reasoning fine-tuning, has been shown to be a key mechanism in reasoning models'enhanced capabilities. Prior work has succeeded in manipulating this behavior via steering vectors, but the underlying mechanism remains poorly understood. In this work, we show that the emergence of backtracking in DeepSeek-R1-Distill-Llama-8B is in part driven by a repurposed direction already present in base model activations. Specifically, we identify a direction in base Llama-3.1-8B's residual stream which systematically induces backtracking when used to steer the distilled reasoning model, and find that the effects of steering with this direction cannot be trivially explained by token-level attributes. We further find that this direction does not induce backtracking in the base model, suggesting that the reasoning finetuning process repurposes pre-existing representations to form new behavioral circuits. Additionally, we hypothesize that this direction is one of several which may work together to mediate backtracking. Our findings offer a compelling picture that reasoning-finetuned models repurpose pre-existing base model representations, rather than learn new capabilities from scratch.", "citations": 10}
{"title": "Understanding Gated Neurons in Transformers from Their Input-Output Functionality", "year": 2025, "authors": "Sebastian Gerstner, Hinrich Sch\u00fctze", "url": "https://www.semanticscholar.org/paper/cb9a2fd28c9c9c6ce00d6d7792410bee6d876fd1", "relevance": 3, "abstract": "Interpretability researchers have attempted to understand MLP neurons of language models based on both the contexts in which they activate and their output weight vectors. They have paid little attention to a complementary aspect: the interactions between input and output. For example, when neurons detect a direction in the input, they might add much the same direction to the residual stream (\"enrichment neurons\") or reduce its presence (\"depletion neurons\"). We address this aspect by examining the cosine similarity between input and output weights of a neuron. We apply our method to 12 models and find that enrichment neurons dominate in early-middle layers whereas later layers tend more towards depletion. To explain this finding, we argue that enrichment neurons are largely responsible for enriching concept representations, one of the first steps of factual recall. Our input-output perspective is a complement to activation-dependent analyses and to approaches that treat input and output separately.", "citations": 0}
{"title": "Feature-Level Insights into Artificial Text Detection with Sparse Autoencoders", "year": 2025, "authors": "Kristian Kuznetsov, Laida Kushnareva, Polina Druzhinina, Anton Razzhigaev, Anastasia Voznyuk, Irina Piontkovskaya, Evgeny Burnaev, Serguei Barannikov", "url": "https://www.semanticscholar.org/paper/c772bea87ba068039adc0d8e139dd314bc0d7851", "relevance": 3, "abstract": "Artificial Text Detection (ATD) is becoming increasingly important with the rise of advanced Large Language Models (LLMs). Despite numerous efforts, no single algorithm performs consistently well across different types of unseen text or guarantees effective generalization to new LLMs. Interpretability plays a crucial role in achieving this goal. In this study, we enhance ATD interpretability by using Sparse Autoencoders (SAE) to extract features from Gemma-2-2b residual stream. We identify both interpretable and efficient features, analyzing their semantics and relevance through domain- and model-specific statistics, a steering approach, and manual or LLM-based interpretation. Our methods offer valuable insights into how texts from various models differ from human-written content. We show that modern LLMs have a distinct writing style, especially in information-dense domains, even though they can produce human-like outputs with personalized prompts.", "citations": 8}
{"title": "Beyond I'm Sorry, I Can't: Dissecting Large Language Model Refusal", "year": 2025, "authors": "Nirmalendu Prakash, Yeo Wei Jie, Amirali Abdullah, Ranjan Satapathy, Erik Cambria, Roy Ka-Wei Lee", "url": "https://www.semanticscholar.org/paper/ec4e56b6fdf1f87272440b0790aaf6e705e8b4b4", "relevance": 3, "abstract": "Refusal on harmful prompts is a key safety behaviour in instruction-tuned large language models (LLMs), yet the internal causes of this behaviour remain poorly understood. We study two public instruction-tuned models, Gemma-2-2B-IT and LLaMA-3.1-8B-IT, using sparse autoencoders (SAEs) trained on residual-stream activations. Given a harmful prompt, we search the SAE latent space for feature sets whose ablation flips the model from refusal to compliance, demonstrating causal influence and creating a jailbreak. Our search proceeds in three stages: (1) Refusal Direction: find a refusal-mediating direction and collect SAE features near that direction; (2) Greedy Filtering: prune to a minimal set; and (3) Interaction Discovery: fit a factorization machine (FM) that captures nonlinear interactions among the remaining active features and the minimal set. This pipeline yields a broad set of jailbreak-critical features, offering insight into the mechanistic basis of refusal. Moreover, we find evidence of redundant features that remain dormant unless earlier features are suppressed. Our findings highlight the potential for fine-grained auditing and targeted intervention in safety behaviours by manipulating the interpretable latent space.", "citations": 4}
{"title": "Learning a Generative Meta-Model of LLM Activations", "year": 2026, "authors": "Grace Luo, Jiahai Feng, Trevor Darrell, Alec Radford, Jacob Steinhardt", "url": "https://www.semanticscholar.org/paper/f00ee65566884274091b003370fe291c6f959d03", "relevance": 3, "abstract": "Existing approaches for analyzing neural network activations, such as PCA and sparse autoencoders, rely on strong structural assumptions. Generative models offer an alternative: they can uncover structure without such assumptions and act as priors that improve intervention fidelity. We explore this direction by training diffusion models on one billion residual stream activations, creating\"meta-models\"that learn the distribution of a network's internal states. We find that diffusion loss decreases smoothly with compute and reliably predicts downstream utility. In particular, applying the meta-model's learned prior to steering interventions improves fluency, with larger gains as loss decreases. Moreover, the meta-model's neurons increasingly isolate concepts into individual units, with sparse probing scores that scale as loss decreases. These results suggest generative meta-models offer a scalable path toward interpretability without restrictive structural assumptions. Project page: https://generative-latent-prior.github.io.", "citations": 0}
{"title": "Attention Sinks: A'Catch, Tag, Release'Mechanism for Embeddings", "year": 2025, "authors": "Stephen Zhang, Mustafa Khan, Vardan Papyan", "url": "https://www.semanticscholar.org/paper/3a3b890fb23e9467861438218be4896ab789e30e", "relevance": 3, "abstract": "Large language models (LLMs) often concentrate their attention on a few specific tokens referred to as attention sinks. Common examples include the first token, a prompt-independent sink, and punctuation tokens, which are prompt-dependent. While the tokens causing the sinks often lack direct semantic meaning, the presence of the sinks is critical for model performance, particularly under model compression and KV-caching. Despite their ubiquity, the function, semantic role, and origin of attention sinks -- especially those beyond the first token -- remain poorly understood. In this work, we conduct a comprehensive investigation demonstrating that attention sinks: catch a sequence of tokens, tag them using a common direction in embedding space, and release them back into the residual stream, where tokens are later retrieved based on the tags they have acquired. Probing experiments reveal these tags carry semantically meaningful information, such as the truth of a statement. These findings extend to reasoning models, where the mechanism spans more heads and explains greater variance in embeddings, or recent models with query-key normalization, where sinks remain just as prevalent. To encourage future theoretical analysis, we introduce a minimal problem which can be solved through the'catch, tag, release'mechanism, and where it emerges through training.", "citations": 3}
{"title": "Graphical Residual Flows", "year": 2022, "authors": "Jacobie Mouton, Steve Kroon", "url": "https://api.semanticscholar.org/CorpusId:248391876", "relevance": 1, "abstract": "Graphical flows add further structure to normalizing flows by encoding non-trivial variable dependencies. Previous graphical flow models have focused primarily on a single flow direction: the normalizing direction for density estimation, or the generative direction for inference. However, to use a single flow to perform tasks in both directions, the model must exhibit stable and efficient flow inversion. This work introduces graphical residual flows, a graphical flow based on invertible residual networks. Our approach to incorporating dependency information in the flow, means that we are able to calculate the Jacobian determinant of these flows exactly. Our experiments confirm that graphical residual flows provide stable and accurate inversion that is also more time-efficient than alternative flows with similar task performance. Furthermore, our model provides performance competitive with other graphical flows for both density estimation and inference tasks.", "citations": 3}
{"title": "Evaluating and Designing Sparse Autoencoders by Approximating Quasi-Orthogonality", "year": 2025, "authors": "Sewoong Lee, Adam Davies, Marc E. Canby, J. Hockenmaier", "url": "https://api.semanticscholar.org/CorpusId:277468308", "relevance": 1, "abstract": "Sparse autoencoders (SAEs) are widely used in mechanistic interpretability research for large language models; however, the state-of-the-art method of using $k$-sparse autoencoders lacks a theoretical grounding for selecting the hyperparameter $k$ that represents the number of nonzero activations, often denoted by $\\ell_0$. In this paper, we reveal a theoretical link that the $\\ell_2$-norm of the sparse feature vector can be approximated with the $\\ell_2$-norm of the dense vector with a closed-form error, which allows sparse autoencoders to be trained without the need to manually determine $\\ell_0$. Specifically, we validate two applications of our theoretical findings. First, we introduce a new methodology that can assess the feature activations of pre-trained SAEs by computing the theoretically expected value from the input embedding, which has been overlooked by existing SAE evaluation methods and loss functions. Second, we introduce a novel activation function, top-AFA, which builds upon our formulation of approximate feature activation (AFA). This function enables top-$k$ style activation without requiring a constant hyperparameter $k$ to be tuned, dynamically determining the number of activated features for each input. By training SAEs on three intermediate layers to reconstruct GPT2 hidden embeddings for over 80 million tokens from the OpenWebText dataset, we demonstrate the empirical merits of this approach and compare it with current state-of-the-art $k$-sparse autoencoders. Our code is available at: https://github.com/SewoongLee/top-afa-sae.", "citations": 2}
{"title": "TSIT: A Simple and Versatile Framework for Image-to-Image Translation", "year": 2020, "authors": "Liming Jiang, Changxu Zhang, Mingyang Huang, Chunxiao Liu, Jianping Shi, Chen Change Loy", "url": "https://api.semanticscholar.org/CorpusId:220713507", "relevance": 1, "abstract": "We introduce a simple and versatile framework for image-to-image translation. We unearth the importance of normalization layers, and provide a carefully designed two-stream generative model with newly proposed feature transformations in a coarse-to-fine fashion. This allows multi-scale semantic structure information and style representation to be effectively captured and fused by the network, permitting our method to scale to various tasks in both unsupervised and supervised settings. No additional constraints (e.g., cycle consistency) are needed, contributing to a very clean and simple method. Multi-modal image synthesis with arbitrary style control is made possible. A systematic study compares the proposed method with several state-of-the-art task-specific baselines, verifying its effectiveness in both perceptual quality and quantitative evaluations.", "citations": 129}
{"title": "A new dance generation task from start to end", "year": 2020, "authors": "Xiao Xu, Lijuan Duan", "url": "https://api.semanticscholar.org/CorpusId:229504762", "relevance": 1, "abstract": "The dance generation task is an interesting task. The general task is to predict the next dance sequence through a given initial frame, or to convert from music to dance through audio information. However, these works cannot control the direction of dance generation and lack suitable ground truth to evaluate the generated results. In our work, we give the start frame and the end frame, which can effectively solve the above problems, and can also be understood as a standardized new task. Our experimental results show that through a two-stream CNN network, we can well complete the new task that we proposed.", "citations": 0}
{"title": "Style-ERD: Responsive and Coherent Online Motion Style Transfer", "year": 2022, "authors": "Tianxin Tao, Xiaohang Zhan, Zhongquan Chen, M. V. D. Panne", "url": "https://api.semanticscholar.org/CorpusId:247292196", "relevance": 1, "abstract": "Motion style transfer is a common method for enriching character animation. Motion style transfer algorithms are often designed for offline settings where motions are processed in segments. However, for online animation applications, such as real-time avatar animation from motion capture, motions need to be processed as a stream with minimal latency. In this work, we realize a flexible, high-quality motion style transfer method for this setting. We propose a novel style transfer model, Style-ERD, to stylize motions in an online manner with an Encoder-Recurrent-Decoder structure, along with a novel discriminator that combines feature attention and temporal attention. Our method stylizes motions into multiple target styles with a unified model. Although our method targets online settings, it outperforms previous offline methods in motion realism and style expressiveness and provides significant gains in runtime efficiency.", "citations": 25}
{"title": "FNet: A Two-Stream Model for Detecting Adversarial Attacks against 5G-Based Deep Learning Services", "year": 2021, "authors": "Guangquan Xu, Guofeng Feng, Litao Jiao, Meiqi Feng, Xi Zheng, Jian Liu", "url": "https://api.semanticscholar.org/CorpusId:237507008", "relevance": 1, "abstract": "With the extensive application of artificial intelligence technology in 5G and Beyond Fifth Generation (B5G) networks, it has become a common trend for artificial intelligence to integrate into modern communication networks. Deep learning is a subset of machine learning and has recently led to significant improvements in many fields. In particular, many 5G-based services use deep learning technology to provide better services. Although deep learning is powerful, it is still vulnerable when faced with 5G-based deep learning services. Because of the nonlinearity of deep learning algorithms, slight perturbation input by the attacker will result in big changes in the output. Although many researchers have proposed methods against adversarial attacks, these methods are not always effective against powerful attacks such as CW. In this paper, we propose a new two-stream network which includes RGB stream and spatial rich model (SRM) noise stream to discover the difference between adversarial examples and clean examples. The RGB stream uses raw data to capture subtle differences in adversarial samples. The SRM noise stream uses the SRM filters to get noise features. We regard the noise features as additional evidence for adversarial detection. Then, we adopt bilinear pooling to fuse the RGB features and the SRM features. Finally, the final features are input into the decision network to decide whether the image is adversarial or not. Experimental results show that our proposed method can accurately detect adversarial examples. Even with powerful attacks, we can still achieve a detection rate of 91.3%. Moreover, our method has good transferability to generalize to other adversaries.", "citations": 3}
{"title": "Characterizing stable regions in the residual stream of LLMs", "year": 2024, "authors": "Jett Janiak, Jacek Karwowski, Chatrik Singh Mangat, Giorgi Giglemiani, Nora Petrova, Stefan Heimersheim", "url": "https://api.semanticscholar.org/CorpusId:272880695", "relevance": 1, "abstract": "We identify stable regions in the residual stream of Transformers, where the model's output remains insensitive to small activation changes, but exhibits high sensitivity at region boundaries. These regions emerge during training and become more defined as training progresses or model size increases. The regions appear to be much larger than previously studied polytopes. Our analysis suggests that these stable regions align with semantic distinctions, where similar prompts cluster within regions, and activations from the same region lead to similar next token predictions. This work provides a promising research direction for understanding the complexity of neural networks, shedding light on training dynamics, and advancing interpretability.", "citations": 2}
{"title": "Dual-Stream Reconstruction Network-Aided ESPRIT Algorithm for DoA Estimation in Coherent Scenarios", "year": 2024, "authors": "Liujie Lv, Sheng Wu, Yi Su, Chunxiao Jiang, Linling Kuang", "url": "https://www.semanticscholar.org/paper/2673bed2e33f901c9f5e9cfe2764620ad903fe66", "relevance": 1, "abstract": "Conventionally, direction of arrival (DoA) estimation of coherent signals relies on pre-processing approaches to eliminate the rank loss of the signal covariance matrix (SCM). However, almost all existing methods utilize the empirical covariance matrix (ECM) as a practical approximation of the SCM, and can not make full use of the information in received signals. In this paper, we propose a deep learning-aided estimation of signal parameters via the rotational invariance technique (ESPRIT) algorithm for DoA estimation in coherent scenarios. Specifically, a dual-stream reconstruction network (DsRNet) is proposed for reconstructing the full-rank SCM directly from received signals, which overcomes the fundamental dependence on the ECM and exploits all available information in received signals. Next, the logarithmic eigenvalue-aided deep residual network (LogDRNet) is proposed for the source number detection, which improves the efficiency of network training and the detection performance, especially with low signal-to-noise ratio (SNR) and limited number of snapshots. Subsequently, we determine the signal subspace using the source number estimated by the LogDRNet and estimate the DoA referring to the ESPRIT algorithm. The proposed deep learning-based framework replaces some crucial aspects of traditional algorithms with customized neural networks, which mitigates the dependence on ideal model assumptions, does not require taking the source number as a prior and improves the estimation performance. Simulation results show that the proposed method not only performs better than existing methods in source number detection and DoA estimation, but also shows superior robustness to various kinds of array imperfections that are widespread in practical systems.", "citations": 7}
{"title": "Analysis of Volleyball Tactics and Movements Based on 3D Spatio-Temporal Residual Network", "year": 2024, "authors": "Bingkui Ma", "url": "https://www.semanticscholar.org/paper/0961a1010616b0ce22ac57739452029d78d321b1", "relevance": 1, "abstract": "Sports technology and 3D motion recognition require models to be able to accurately identify athletes\u2019 movements, which is crucial for training analysis, game strategy development and refereeing assistance decisions. To maintain a high recognition rate under different competition scenes, different athlete styles and different environmental conditions, and to ensure the practicality and reliability of the model, two independent 3D convolutional neural networks are applied to construct the action recognition model of Two-stream 3D Residual Networks. According to the temporal-spatial characteristics of human movements in video, the model introduces attention mechanism and combines time dimension to build a Two-stream 3D Residual Networks action recognition model integrating time-channel attention. The average accuracy of Top-1 and Top-5 action recognition models of Two-stream 3D Residual Networks integrated with pre-activation structure is 68.97% and 91.68%. The residual block of the pre-activated structure can enhance the model\u2019s effectiveness. The average precision of Top-1 and Top-5 of action recognition model of two-stream 3D spatio-temporal residual network integrating time-channel attention is 85.73% and 92.05%, which has higher accuracy. The action recognition model of the two-stream 3D spatio-temporal residual network, which integrates time-channel attention, is accurate and achieves good recognition results with volleyball action recognition in real scenes.", "citations": 1}
{"title": "Residual learning-based two-stream network for RGB-T object tracking", "year": 2022, "authors": "Yili Chen, Minjie Wan, Yunkai Xu, Xiaojie Zhang, Qian Chen, G. Gu", "url": "https://www.semanticscholar.org/paper/8fc89b11842c1b896f94f785d6b50040e0cab767", "relevance": 1, "abstract": "Abstract. RGB-T object tracking is a branch of visual tracking that has been widely applied to many fields, such as intelligent transportation and urban monitoring. Due to the interference of background clutter and occlusion, the existing trackers still suffer from the problems of unreasonable modal fusion strategy, insufficient feature extraction, and loss of semantic information. To solve these problems, we propose a residual learning-based two stream network for RGB-T object tracking. The overall feature extraction network is composed of three branches, and multi-layer convolutions are utilized to extract the features of visible, thermal, and fused images, respectively. First, aiming at improving the effectiveness of feature extraction, a weight generation module for hierarchical feature weight calculation is designed to guide the direction of feature fusion. Then, the residual block is employed to replace the single-layer convolution in order to increase the depth of the network, by which deeper semantic features are learned and the loss of semantic information is alleviated. Finally, a loss function with a penalty term is developed to adjust our network toward the direction of the best tracking performance of the dual modalities. This overcomes the negative impact of poor mode on model training. Experiments implemented on public RGB-T datasets indicate that our algorithm outperforms the recent state-of-the-art trackers in terms of both precision rate and success rate. Compared with the second-best comparison algorithm, our tracker improves the above two metrics by 0.4% and 1.5%, respectively. Our codes are available at https://github.com/MinjieWan/Residual-learning-based-two-stream-network-for-RGB-T-object-tracking.", "citations": 1}
{"title": "Research on the Multilabel Data Stream Classification Method Based on Fuzzy Complex Set-Valued Measure Learning", "year": 2023, "authors": "Haicai Wu, Dawei Yun, F. Gu", "url": "https://www.semanticscholar.org/paper/5501f127fe7810df4071880d045c8440499a0b04", "relevance": 1, "abstract": "In the process of modern industrial production equipment developing towards the direction of structure, automation, and intelligence, motor is still the main power output equipment. If the data flow classification occurs during the operation of the motor, it will lead to problems such as the reduction of its operation efficiency and the increase in system energy consumption. In serious cases, it will even cause motor damage, and the overall system equipment will be shut down for maintenance for a long time, resulting in serious economic losses. Therefore, the research on intelligent multilabel data stream classification technology of motor is of great significance to ensure the stability and reliability of efficient operation of production equipment. In order to improve the recognition efficiency and accuracy of ms-1dcnn\u2019s multilabel data stream classification method in the environment of variable motor conditions and strong noise interference, a multiscale feature fusion framework is constructed based on the residual network structure. The implementation principles of two kinds of attention mechanism algorithms, squeeze and excitation module and convolution attention module, are studied, respectively. The attention module suitable for one-dimensional residual network is designed and embedded into the residual module to build a multiscale attention residual network model. Finally, the effectiveness and superiority of the proposed model are verified by using the experimental platform data.", "citations": 0}
{"title": "Fashion Landmark Detection via Deep Residual Spatial Attention Network", "year": 2021, "authors": "Rui Wang, Jun Feng, Qirong Bu", "url": "https://www.semanticscholar.org/paper/312530773e3a0dec952c9b8eea263a488575fdb7", "relevance": 1, "abstract": "Fashion landmark detection is challenging due to the large spatial variances of the landmarks and the scale variations of the clothing images. Therefore, the fashion landmark detection model requires two abilities for accurately locating the coordinates of the landmarks. One is to autonomously focus on the task-related features of clothing images to adapt to the diverse spatial distribution of the fashion landmarks. The other is to extract features that contain multi-scale context information to deal with the scale variations. For these purposes, first, we propose the Direction-aware Spatial Attention Module(DASM), which embeds the direction-aware information into spatial attention to capture global contexts and help the network enhance features. And we integrate our DSAM with a bottleneck to building a Spatial Attention ResBlock(SARB). Based on the SARB, we establish a residual-style network for the feature extraction. Then, two feature fusion operations are performed for encoding multi-scale contexts, which are the feature refinement in a top-to-down way and the multi-scale feature aggregation. We name the proposed model the Deep Residual Spatial Attention Network. We demonstrate the effectiveness of our proposed method by the experimental results on two benchmark datasets, which show the proposed fashion landmark detection network outperforms the state-of-the-art methods.", "citations": 1}
