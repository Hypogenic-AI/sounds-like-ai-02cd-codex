\section*{Abstract}
\label{sec:abstract}
Large language model (LLM) outputs often "sound like AI," but it is unclear whether this style corresponds to a simple, linearly encoded direction in the residual stream. We study this question using \hcThree, which provides paired human and ChatGPT answers, and a small modern LLM (\qwen). Our approach probes residual-stream activations at multiple layers with a linear classifier and constructs a mean-difference steering direction between AI and human responses. We then add or remove this direction during generation and measure shifts in AI-likeness using both the probe and a \gptfourone judge. On a balanced pilot set (600 samples), linear probes separate AI vs human text with high accuracy, peaking at 97.8\% at layer 12. Steering at this discriminative layer shifts probe-based AI-likeness upward for AI-plus and downward for AI-minus (mean changes +0.048 and -0.156, respectively), with consistent but statistically non-significant effects in this small run. These results suggest that "AI-sounding" style is at least partially linearly represented in residual activations and can be nudged via simple edits, while highlighting the need for larger, multi-model studies to confirm robustness and practical significance.
